<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>My Blogs - Training Mario with Reinforcement Learning</title>
    <link rel="stylesheet" href="../css/blog-styles.css">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <style>
        .dropdown-heading {
            cursor: pointer;
            font-weight: bold;
            padding: 10px;
            padding-left: 30px; /* Add space for triangle on the left */
            border: 1px solid #ddd;
            background-color: #f9f9f9;
            margin: 0;
            position: relative;
        }

        /* Triangle icon using ::before pseudo-element */
        .dropdown-heading::before {
            content: '';
            border-style: solid;
            border-width: 5px 5px 0 5px; /* Triangle pointing down */
            border-color: #333 transparent transparent transparent;
            position: absolute;
            left: 10px; /* Position triangle on the left */
            top: 50%;
            transform: translateY(-50%);
            transition: transform 0.3s; /* Smooth rotation */
        }

        .dropdown-content {
            display: none;
            padding: 10px;
            border: 1px solid #ddd;
            border-top: none;
        }

        /* Show content and rotate triangle when open */
        .dropdown.open .dropdown-content {
            display: block;
        }

        /* Rotate the triangle to point up */
        .dropdown.open .dropdown-heading::before {
            transform: translateY(-50%) rotate(180deg);
        }
        .note {
            color: #888;
            font-style: italic;
        }
    </style>
</head>

<body>
    <aside class="sidebar">
        <div class="header-content">
            <img id="logo" src="../images/doodle_circle.png" alt="Logo">
            <div class="text-content">
                <h1>Duc Phat Nguyen</h1>
                <h3>A student looking for opportunities in Data Science & AI</h3>
            </div>
        </div>
        <nav id="nav-bar">
            <a href="../index.html">Homepage</a>
            <a href="../portfolio.html">My Portfolio</a>
            <a href="../blog.html" class="current">My Blogs</a>
        </nav>
        <div class="sidebar-extra">
            <p><a href="mailto:phatnguyenground@gmail.com"><i class="fa-solid fa-at"></i></a>
                <a href="https://github.com/ethannguyen2k" target="_blank"><i class="fa-brands fa-github-alt"></i></a>
                <a href="https://linkedin.com/in/ethan-2k" target="_blank"><i class="fab fa-linkedin"></i></a></p>
        </div>
    </aside>

    <div class="container2 fade-in">
        <div>
            <h2>Training Mario with Reinforcement Learning</h2>
            <div class="blog-info">
                <p>Date Created: <span id="date-created">November 12, 2024</span></p>
                <p>Date Modified: <span id="date-modified"></span></p>
            </div>
            <img src="mario-images/mario-image.jpg" alt="Project header image - Mario" style="display: block; margin-left: auto; margin-right: auto; width: 85%;">
            <p>In a lovely day, I asked myself, how can I make a computer learn to play Mario? Well, I did just that, and I embarked on this journey to understand reinforcement learning (RL) better. This blog post documents my experiments with training an AI agent to play Super Mario Bros using a Double Deep Q-Network (DDQN).</p>

        <h3>What is Reinforcement Learning?</h3>
        <p>Reinforcement learning (RL) is a type of machine learning where an agent learns by interacting with its environment to achieve a specific goal. 
            The agent takes actions and receives rewards or penalties based on the effectiveness of those actions. 
            Over time, the agent uses this feedback to adjust its behavior, aiming to maximize cumulative rewards. 
        </p>
        <!-- #TODO: Add a diagram here -->
        <p>RL is commonly used in gaming, robotics, finance, or any setting where sequential decision-making under uncertainty is required. 
            For example, an RL agent can learn to play video games by trying different strategies and learning from the outcomes. 
            A famous example is AlphaGo, which used RL to beat world champions in <a href="https://deepmind.google/research/breakthroughs/alphago/">the game of Go</a>. There are also power agents in games like <a href="https://deepmind.google/discover/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii/">AlphaStar in StarCraft</a> and <a href="https://openai.com/index/openai-five-defeats-dota-2-world-champions/">OpenAI Five in Dota 2</a> to play at superhuman levels.
        </p>
        <p>In the case of Mario, the agent's actions are moving left or right, jumping, or shooting fireballs.</p>
        <p>The basic elements of reinforcement learning include:</p>
        <ol>
            <li><b>Agent</b>: The learner or decision-maker that interacts with the environment.</li>
            <li><b>Environment</b>: The setting in which the agent operates.</li>
            <li><b>Action</b>: Choices the agent can make.</li>
            <li><b>State</b>: The current situation or context in which the agent finds itself.</li>
            <li><b>Reward</b>: Feedback given to the agent to indicate success or failure.</li>
        </ol>
        <img src="mario-images/mario-diagram.png" alt="Agent-environment interaction loop" style="display: block; margin-left: auto; margin-right: auto; width: 50%;">
        <p>"For any given state, an agent can choose to do the most optimal action (exploit) or a random action (explore)."
            This is something that agent has to learn to make better decision. 
            This trade-off between exploration and exploitation is a key challenge in reinforcement learning.
        </p>

        <div class="dropdown">
            <h3 class="dropdown-heading" onclick="toggleDropdown(this)">Core Concepts</h3>
            <div class="dropdown-content">
                <img src="https://spinningup.openai.com/en/latest/_images/rl_diagram_transparent_bg.png" alt="Agent-environment interaction loop" style="display: block; margin-left: auto; margin-right: auto;">
                <p>Imagine you're playing a video game. You are the <b>agent</b> (the character you control), and the environment is the game world (the levels, obstacles, and everything around you).
                    You can see things happening in the game (like a monster coming toward you) and decide what to do next (maybe jump, run away, or fight back), which is the <b>environment</b>.
                </p>
                <h4>States and Observations</h4>
                <p>The <b>state</b> is like a snapshot of everything in the game world at a specific moment. For example, where the monsters are, what items are around, and how much health you have.
                    <b>Observations</b> are what the agent can see or know about the world. Sometimes you can see everything (like in a game where you can see the whole map), and sometimes you can only see part of it (like if you're inside a building in the game and can't see the outside).
                    This means the state is the complete picture (fully observed if agent can see the state) while the agent can only see part of it (partially observed, agent can only see a partial observation), most of the time.
                </p>
                <h4>Action Space</h4>
                <p><b>Action space</b> is the set of all possible things you can do in the game. 
                    For example, you might have options like "jump", "run", or "attack". 
                    These are the actions the agent can choose from.
                </p>
                <p>In some environments, such as Atari games and Go, the agent operates within <b>discrete action spaces</b>, meaning there are a limited number of possible actions it can take. 
                    Other environments, like where the agent controls a robot in a physical world, have <b>continuous action spaces</b>. 
                    In continuous spaces, actions are real-valued vectors.</p>
                <h4>Policies</h4>
                <p>A <b>policy</b> is like a set of rules or a plan that tells the agent what to do based on what it sees. 
                    So, if you see a monster, your policy might tell you to jump, or if it is attacking you first, you fight back. 
                    If you see a treasure, your policy might tell you to collect it.
                </p>
                <p>Since the policy acts as the agent's brain, the terms "policy" and "agent" are often used interchangeably. For example, one might say, "The policy aims to maximize the reward."</p>
                <p>In deep RL, we use parameterized policies. These policies are functions whose outputs depend on a set of parameters, such as the weights and biases of a neural network. 
                    In adjusting these parameters through optimization algorithms, we can change the behavior of the policy.</p>
                <p>A policy can be either deterministic or stochastic.</p>
                <!-- see if deter and sto are like exploit and explore -->
                <h4>Trajectories</h4>
                <p>A <b>trajectory</b> is a sequence of states and actions in the world, like the path the agent takes through the game. 
                    It's the series of actions the agent takes, starting from the beginning until the end. 
                    It's like the story of what you do in the game.
                </p>
                <p>Trajectories are also frequently called episodes or rollouts.</p>
                <h4>Rewards</h4>
                <p>The goal of Reinforcement Learning (RL) is for <i><u>the agent to get better by practicing and learning from what happens when it takes different actions</u></i>. 
                    The agent gets rewards for good actions and tries to do more of those to maximize its total reward.
                </p>
                <p>The reward function, written as ùëÖ
                    , tells the agent how good or bad a certain action was. This depends on:
                    Where the agent was (the state it was in, like a level in a game),
                    What action it just took (like moving, jumping, or grabbing something),
                    Where it ended up (the next state, after the action).
                    So we could write it as:
                    ùëÖ=ùëÖ(ùë†,ùëé,ùë†‚Ä≤)
                    </p>
                <p>Sometimes, though, it's simpler to look only at where the agent is now (or where it is and what action it took).</p>
                <p>The agent doesn‚Äôt just care about the reward from one action‚Äîit cares about getting the most points or rewards over time.
                    This total score over a period is called the return, written as R(œÑ). There are a couple of ways to look at return:
                    <ul>
                        <li><b>Finite-Horizon Undiscounted Return</b>: adding up all the points you get in a short level or time period in a game.</li>
                        <li><b>Infinite-Horizon Discounted Return</b>: look at all rewards the agent ever gets (an infinite horizon), but we use a discount factor (written as Œ≥) to make future rewards worth a bit less.</li>
                    </ul>
                </p>
                <p>Prioritize short-term gains (like going for treasures) but still consider discounted long-term rewards (by factoring in future treasures after defeating the monster). Plus, using a discount factor makes the math easier. 
                    For each decision, it might calculate a value estimate based on both immediate rewards and discounted future rewards. 
                    So, even if it avoids one monster initially for a quick treasure, it might tackle the next if it seems worthwhile.</p>
            </div>
        </div>

        <h3>Initial Setup</h3>
        <p><span class="note">Note: This blog follows the instructions from <a href="https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html">this tutorial</a>. 
            Discussion and modifications that entails will be attempted to make sense of the article and the code.</span></p>
        <p>Setting up the environment was quite the adventure. 
            If you've worked with Python packages before, you know the usual suspects - version conflicts, deprecation warnings, and the occasional "this doesn't work like it used to" moments. 
        </p>
        
        <ul>
            <li>Outdated components</li>
            <li>Compatibility issues because some functions are deprecated</li>
            <li>Getting the right combination of dependencies' versions</li>
        </ul>
        <p>After a few hours of debugging and package juggling, I finally got everything working together.</p>

        <p>To list the whole dependencies I used would be a bit much, but here are the main ones:</p>
        <div class="code-window">
            <div class="console">
                <div class="red"></div>
                <div class="orange"></div>
                <div class="green"></div>
            </div>
        <pre class="code-content"><code>
pytorch=2.4.1=py3.8_cuda12.4_cudnn9_0
torchrl=0.5.0
torchvision=0.20.0
gym=0.26.1
gym-super-mario-bros=7.4.0
numpy=1.24.4
matplotlib-base=3.7.3
        </code></pre>
        </div>

        <p>You can see that I used CUDA and cuDNN for GPU acceleration.
            To find the right versions, I recommend checking <a href="https://stackoverflow.com/a/55717476">here</a>, <a href="https://pytorch.org/get-started/locally/">the official PyTorch website</a>, or using miniconda conda-forge channel for up-to-date packages.
            Once you have the right versions, it will look something like this:
        </p>
        <img src="mario-images/cuda.png" alt="CUDA device confirmation" style="display: block; margin-left: auto; margin-right: auto;">

        <h3>Initialise Environment</h3>
        <p>In Mario, tubes, mushrooms, etc... are the components of the environment. 
            This is where the agent will interact with the game world, taking actions and receiving rewards based on its performance.
        </p>

        <div class="code-window">
            <div class="console">
                <div class="red"></div>
                <div class="orange"></div>
                <div class="green"></div>
            </div>
        <pre class="code-content"><code>
# Initialize Super Mario environment (in v0.26 change render mode to 'human' to see results on the screen)
if gym.__version__ < '0.26':
    env = gym_super_mario_bros.make("SuperMarioBros-1-1-v0", new_step_api=True)
else:
    env = gym_super_mario_bros.make("SuperMarioBros-1-1-v0", render_mode='rgb', apply_api_compatibility=True)

# Define the movement options for Super Mario
MOVEMENT_OPTIONS = [
    ["right"],        # Move right
    ["right", "A"],   # Jump right
]

# Apply the wrapper to the environment
env = JoypadSpace(env, MOVEMENT_OPTIONS)

env.reset()
next_state, reward, done, trunc, info = env.step(action=0)
print(f"{next_state.shape},\n {reward},\n {done},\n {info}")
        </code></pre>
        </div>

        <p>This code snippet initializes the Super Mario environment, sets up the movement options, and applies a wrapper to the environment, which will helps the agent interact with the environment by providing a set of actions it can take.
            As a test, the code prints out the next state, reward, done flag, and info after taking the first action in the environment.
        </p>

        <div class="output-cell">
            <div class="output-line">(240, 256, 3), </div>
            <div class="output-line">0.0, </div>
            <div class="output-line">False, </div>
            <div class="output-line">{'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}</div>
        </div>
        
        <p>When you call env.step(action=0), you're telling Mario to perform action 0, which from the MOVEMENT_OPTIONS list is moving right. Of course, you can change the action to 1, which is a jump right.
            The function returns the next state (a 240x256x3 image), the reward (0.0 in this case), a done flag (False, meaning the episode is not over), and some additional information about the environment.
            This is something that Mario will probably see (for illustration purposes):
        </p>
        <img src="mario-images/mario-screen.svg" alt="Mario screen" style="display: block; margin-left: auto; margin-right: auto; width: 45%;">
        <p>Think of it like pressing the right button for a split second, checks if you get a reward, game over or not, and then returns the new state of the game.
            This is a single step in the game, the interaction loop will continue until the game is over or the AI beat the game.
        </p>

        <h3>Pre-process the Environment</h3>

        <p>In the previous output, the next state was a 240x256x3 image which is returned by the environment.
            Often, this is too much information for the agent to process directly.
            Mario does not need to see the entire screen to make decisions.
            Instead, we will apply wrappers to the environment to pre-process the images and make them more manageable for the agent.
        </p>
        <img src="mario-images/env-wrapper.png" alt="Environment wrapper" style="display: block; margin-left: auto; margin-right: auto;">
        <img src="mario-images/frame-transformation.svg" alt="Frame transformation" style="display: block; margin-left: auto; margin-right: auto; width: 70%;">

        <p>The final output is smaller by almost 85%, which means faster processing and less memory usage.
            This is much simpler than what a human sees, but contains all the essential information needed to learn how to play the game effectively.           
        </p>

        <h3>Replay Buffer</h3>

        <p>A replay buffer is like a "memory bank" that stores the agent's experiences while it plays the game. Each experience consists of:
            The current state (what Mario sees),
            the action taken,
            the reward received,
            the next state,
            and whether the game ended (done flag). Something like this:
        </p>
        <img src="mario-images/replay-buffer-structure.svg" alt="Replay buffer structure" style="display: block; margin-left: auto; margin-right: auto; width: 70%;">

        <p>In <a href="https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html">this tutorial</a>, the author uses <b>TensorDictReplayBuffer with LazyMemmapStorage</b>, which is a custom replay buffer implementation.
            The replay buffer stores experiences (state, action, reward, next state, done) and samples mini-batches for training the agent.
            Only, I failed to get it running.
        </p>
        <div class="output-cell">
            <div class="output-line">OSError: [WinError 1455] The paging file is too small for this operation to complete</div>
        </div>
        <p>Apparently, this is a Windows-specific error occurs when the system tries to allocate more virtual memory than is available.
            Since I don't want to mess with the system too much and I don't have any money for RAM yet, I decided to use a simpler replay buffer implementation and called it SimpleReplayBuffer.
        </p>

        <table>
            <thead>
                <tr>
                    <th>SimpleReplayBuffer</th>
                    <th>TensorDictReplayBuffer with LazyMemmapStorage</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Uses simple Python deque for storage</td>
                    <td>More sophisticated memory management</td>
                </tr>
                <tr>
                    <td>Implements basic prioritized experience replay</td>
                    <td>Uses memory mapping for large datasets</td>
                </tr>
                <tr>
                    <td>Straightforward memory management</td>
                    <td>More complex data structures</td>
                </tr>
                <tr>
                    <td>Less feature-rich but more robust</td>
                    <td>More features but more potential points of failure</td>
                </tr>
            </tbody>
        </table>
        <img src="mario-images/memory-usage.svg" alt="Memory usage comparison" style="display: block; margin-left: auto; margin-right: auto; width: 80%;">
        <p>My <b>SimpleReplayBuffer</b> does 2 key things:</p>
        <ol>
            <li>push (Storing Experiences): 
                <ul>
                    <li>It takes a snapshot of what happened during one step of Mario's gameplay.</li>
                    <li>It calculates how important this memory is (priority = |reward| +  epsilon).</li>
                    <li>It stores the experience in a deque and the priority in a separate deque.</li>
                </ul>
            </li>
            <li>sample (Prioritized Sampling): 
                <ul>
                    <li>Think of this like Mario "remembering" past experiences to learn from. He will takes in how many memories to recall (batch_size) and prefers to remember important moments (high rewards).</li>
                    <li>If picking importance memories failed (due to zero probabilities), it will fallback to random memories.</li>
                    <li>Then, it converts chosen memories to format for learning (GPU tensors).</li>
                </ul>
            </li>
        </ol>
        <p>Even though it is essentially the same with TensorDictReplayBuffer, the <b>SimpleReplayBuffer</b> is actually a better solution for my current setup. 
            It is lightweight, easy to understand, and doesn't require any special dependencies.
            Plus, it is easier to debug and modify if needed.
        </p>

        <h3>It's-a me, Mario!</h3>
        <p>It's time create the Mario-playing agent. Basically, Mario should be able to:
        </p>
        <ul>
            <li><b>Act</b> according to the optimal policy (exploit) or try new things (explore) based on the current state.</li>
            <li><b>Remember</b> experiences. Experience = (current state, current action, reward, next state). 
                Mario will <b>cache</b> and <b>recall</b> these experiences to update his policy.</li>
            <li><b>Learn</b> from these experiences, which means updating his policy to maximize rewards over time.</li>
        </ul>

        <div class="dropdown">
            <h3 class="dropdown-heading" onclick="toggleDropdown(this)">Act</h3>
            <div class="dropdown-content">
                <div class="code-window">
                    <div class="console">
                        <div class="red"></div>
                        <div class="orange"></div>
                        <div class="green"></div>
                    </div>
                <pre class="code-content"><code>
def act(self, state):
    """
    Given a state, choose an epsilon-greedy action and update value of step.

    Inputs:
    state(``LazyFrame``): A single observation of the current state, dimension is (state_dim)
    Outputs:
    ``action_idx`` (``int``): An integer representing which action Mario will perform
    """
                </code></pre>
                </div>

                <p>The act function takes a state, which is a LazyFrame object representing what Mario "sees" at a given moment.
                    From there, it chooses an action based on an epsilon-greedy strategy (exploit vs. explore).
                    If the agent is exploring (randomly trying new things), it will choose a random action.
                    If the agent is exploiting (using what it knows to maximize rewards), it will choose the best action by relying on MarioNet in the Learn section.
                </p>

                <img src="mario-images/act-method-flow.svg" alt="Act method flow" style="display: block; margin-left: auto; margin-right: auto; width: 70%;">
                <p>The method is called every time Mario needs to make a decision, which happens many times per second during gameplay.
                </p>

                <p>Exploration rate early on is 1, meaning Mario will explore 100% of the time early in training.
                    Over time, this rate will decrease through <code>exploration_rate_decay</code>, so Mario will explore less and force to exploit more.
                    This value will never goes below <code>exploration_rate_min</code>, which can be set.
                </p>
            </div>
        </div>
        
        <div class="dropdown">
            <h3 class="dropdown-heading" onclick="toggleDropdown(this)">Remember (Cache and Recall)</h3>
            <div class="dropdown-content">
                <div class="code-window">
                    <div class="console">
                        <div class="red"></div>
                        <div class="orange"></div>
                        <div class="green"></div>
                    </div>
                <pre class="code-content"><code>
def cache(self, state, next_state, action, reward, done):
    """
    Store the experience to self.memory (replay buffer)

    Inputs:
    state (``LazyFrame``),
    next_state (``LazyFrame``),
    action (``int``),
    reward (``float``),
    done(``bool``)
    """

def recall(self):
    """
    Retrieve a batch of experiences from memory
    """
                </code></pre>
                </div>
            </div>
        </div>

        <div class="dropdown">
            <h3 class="dropdown-heading" onclick="toggleDropdown(this)">Learn</h3>
            <div class="dropdown-content">
            
            </div>
        </div>

        <h3>Understanding the Network Architecture</h3>
        <p>I started with a PyTorch tutorial on training a Mario-playing RL agent, but let's break down what's actually happening here. The model uses a Double Deep Q-Network (DDQN), which is an improvement over standard DQN.</p>

        <div class="code-window">
            <div class="console">
                <div class="red"></div>
                <div class="orange"></div>
                <div class="green"></div>
            </div>
        <pre class="code-content"><code>
def __build_cnn(self, c, output_dim):
    return nn.Sequential(
        nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),
        nn.ReLU(),
        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),
        nn.ReLU(),
        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),
        nn.ReLU(),
        nn.Flatten(),
        nn.Linear(3136, 512),
        nn.ReLU(),
        nn.Linear(512, output_dim),
    )
        </code></pre>
        </div>

        <h3>Additional Parameters</h3>

        <h3>Training Process and Hyperparameter Tuning</h3>
        <p>The training process was quite extensive. With my current setup:</p>

        <table>
            <thead>
                <tr>
                    <th>Parameter</th>
                    <th>Original</th>
                    <th>Modified</th>
                    <th>Reasoning</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Batch size</td>
                    <td>32</td>
                    <td>256</td>
                    <td>More stable learning</td>
                </tr>
                <tr>
                    <td>Exploration rate min</td>
                    <td>0.1</td>
                    <td>0.15</td>
                    <td>Maintain exploration</td>
                </tr>
                <tr>
                    <td>Exploration decay</td>
                    <td>0.99999975</td>
                    <td>0.99999</td>
                    <td>Faster decay</td>
                </tr>
                <tr>
                    <td>Gamma</td>
                    <td>0.9</td>
                    <td>0.98</td>
                    <td>Better long-term planning</td>
                </tr>
                <tr>
                    <td>Learning rate</td>
                    <td>0.00025</td>
                    <td>0.0005</td>
                    <td>Faster learning</td>
                </tr>
                <tr>
                    <td>Memory size</td>
                    <td>100000</td>
                    <td>50000</td>
                    <td>Focus on recent experiences</td>
                </tr>
            </tbody>
        </table>

        <p>One interesting addition was implementing a prioritized replay buffer:</p>
        <div class="code-window">
            <div class="console">
                <div class="red"></div>
                <div class="orange"></div>
                <div class="green"></div>
            </div>
        <pre class="code-content"><code>
class SimpleReplayBuffer:
    def __init__(self, capacity, device):
        self.buffer = deque(maxlen=capacity)
        self.priorities = deque(maxlen=capacity)
        self.device = device
        self.eps = 1e-5  # Avoid zero probabilities

    def push(self, state, next_state, action, reward, done):
        priority = float(abs(reward)) + self.eps
        self.buffer.append((state, next_state, action, reward, done))
        self.priorities.append(priority)
        </code></pre>
        </div>

        <p>I will continue to update this post in the future!</p>
        </div>

        <section id="references">
        <h2>References</h2>
        <ol>
            <li>
            Doe, J., & Smith, A. (2023). <em>Understanding Machine Learning</em>. Tech Journal, 10(4), 102-112. Available at: <a href="https://example.com">https://example.com</a>
            </li>
            <li>
            Miller, R. (2022). "Optimizing Manufacturing Processes." <em>Industrial Insights</em>. Retrieved from <a href="https://industrialinsights.com/article">https://industrialinsights.com/article</a>
            </li>
            <li>
            Chen, L., et al. (2021). "A Comprehensive Guide to Data Science." <em>Data Science Review</em>, 5(2), 45-60. DOI: <a href="https://doi.org/10.1234/dsr.2021.4567">10.1234/dsr.2021.4567</a>
            </li>
            <li>
            Brown, T. (2020). <em>Artificial Intelligence for Beginners</em>. New York: AI Publishing.
            </li>
        </ol>
        </section>
          
    <p><a href="index.html">Back to Home</a></p>

    <footer>
        <hr>
        <p>Made by Ethan, 2024, with üíú. <a href="mailto:phatnguyenground@gmail.com">Email me</a></p>
    </footer>
</div>

</body>
<script src="../js/scripts.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>
<script>
// JavaScript to set the date modified
    document.getElementById('date-modified').textContent = document.lastModified;
    function toggleDropdown(element) {
    element.parentElement.classList.toggle("open");
  };
</script>
</html>    