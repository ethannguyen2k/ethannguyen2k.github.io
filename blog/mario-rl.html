<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>My Blogs - Training Mario with Reinforcement Learning</title>
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <link rel="stylesheet" href="../css/blog-styles.css">
    <script>
        // Immediately apply the saved theme
        (function() {
            const savedTheme = localStorage.getItem('theme') || 'light';
            document.documentElement.setAttribute('data-theme', savedTheme);
        })();
    </script>
    <style>
        .dropdown-heading {
            cursor: pointer;
            font-weight: bold;
            padding: 10px;
            padding-left: 30px; /* Add space for triangle on the left */
            border: 1px solid var(--border-color);
            background-color: var(--bg-secondary);
            margin: 0;
            position: relative;
        }

        /* Triangle icon using ::before pseudo-element */
        .dropdown-heading::before {
            content: '';
            border-style: solid;
            border-width: 5px 5px 0 5px; /* Triangle pointing down */
            border-color: var(--text-primary) transparent transparent transparent;
            position: absolute;
            left: 10px; /* Position triangle on the left */
            top: 50%;
            transform: translateY(-50%);
            transition: transform 0.3s; /* Smooth rotation */
        }

        .dropdown-content {
            display: none;
            padding: 10px;
            border: 1px solid #ddd;
            border-top: none;
        }

        /* Show content and rotate triangle when open */
        .dropdown.open .dropdown-content {
            display: block;
        }

        /* Rotate the triangle to point up */
        .dropdown.open .dropdown-heading::before {
            transform: translateY(-50%) rotate(180deg);
        }
        .note {
            color: #888;
            font-style: italic;
        }
    </style>
</head>

<body>
    <aside class="sidebar">
        <div class="header-content">
            <img id="logo" src="../images/logo2.webp" alt="Logo">
            <div class="text-content">
                <h1>hi! i'm ethan</h1>
                <h3>hard learning. hardworking. hard playing. ready for what's next</h3>
            </div>
        </div>
        <nav id="nav-bar">
            <a href="../index.html">Homepage</a>
            <a href="../portfolio.html">My Portfolio</a>
            <a href="../blog.html" class="current">My Blogs</a>
        </nav>
        <div class="theme-toggle">
            <button class="theme-btn" id="themeToggle" aria-label="Toggle theme">
                <i class="fa-solid fa-sun"></i>
                <i class="fa-solid fa-moon"></i>
            </button>
        </div>
        <div class="sidebar-extra">
            <p><a href="mailto:phatnguyenground@gmail.com"><i class="fa-solid fa-at"></i></a>
                <a href="https://github.com/ethannguyen2k" target="_blank"><i class="fa-brands fa-github-alt"></i></a>
                <a href="https://linkedin.com/in/ethan-2k" target="_blank"><i class="fab fa-linkedin"></i></a></p>
        </div>
    </aside>

    <div class="container2 fade-in">
        <div class="blog-content">
            <h2>Training Mario with Reinforcement Learning</h2>
            <div class="blog-info">
                <p>Date Created: <span id="date-created">November 12, 2024</span></p>
                <p>Date Modified: <span id="date-modified"></span></p>
            </div>
            <img src="mario-images/mario-image.jpg" alt="Project header image - Mario" style="display: block; margin-left: auto; margin-right: auto; width: 85%;">
            <p>In a lovely day, I asked myself, how can I make a computer learn to play Mario? Well, I did just that, and I embarked on this journey to understand reinforcement learning (RL) better. This blog post documents my experiments with training an AI agent to play Super Mario Bros using a Double Deep Q-Network (DDQN).</p>

        <h3>What is Reinforcement Learning?</h3>
        <p>Reinforcement learning (RL) is a type of machine learning where an agent learns by interacting with its environment to achieve a specific goal. 
            The agent takes actions and receives rewards or penalties based on the effectiveness of those actions. 
            Over time, the agent uses this feedback to adjust its behavior, aiming to maximise cumulative rewards. 
        </p>
        <!-- #TODO: Add a diagram here -->
        <p>RL is commonly used in gaming, robotics, finance, or any setting where sequential decision-making under uncertainty is required. 
            For example, an RL agent can learn to play video games by trying different strategies and learning from the outcomes. 
            A famous example is AlphaGo, which used RL to beat world champions in <a href="https://deepmind.google/research/breakthroughs/alphago/">the game of Go</a>. There are also power agents in games like <a href="https://deepmind.google/discover/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii/">AlphaStar in StarCraft</a> and <a href="https://openai.com/index/openai-five-defeats-dota-2-world-champions/">OpenAI Five in Dota 2</a> to play at superhuman levels.
        </p>
        <p>In the case of Mario, the agent's actions are moving left or right, jumping, or shooting fireballs.</p>
        <p>The basic elements of reinforcement learning include:</p>
        <ol>
            <li><b>Agent</b>: The learner or decision-maker that interacts with the environment.</li>
            <li><b>Environment</b>: The setting in which the agent operates.</li>
            <li><b>Action</b>: Choices the agent can make.</li>
            <li><b>State</b>: The current situation or context in which the agent finds itself.</li>
            <li><b>Reward</b>: Feedback given to the agent to indicate success or failure.</li>
        </ol>
        <img src="mario-images/mario-diagram.png" alt="Agent-environment interaction loop" style="display: block; margin-left: auto; margin-right: auto; width: 50%;">
        <p>"For any given state, an agent can choose to do the most optimal action (exploit) or a random action (explore)."
            This is something that agent has to learn to make better decision. 
            This trade-off between exploration and exploitation is a key challenge in reinforcement learning.
        </p>

        <div class="dropdown">
            <h3 class="dropdown-heading" onclick="toggleDropdown(this)">Core Concepts</h3>
            <div class="dropdown-content">
                <img src="https://spinningup.openai.com/en/latest/_images/rl_diagram_transparent_bg.png" alt="Agent-environment interaction loop" style="display: block; margin-left: auto; margin-right: auto;">
                <p>Imagine you're playing a video game. You are the <b>agent</b> (the character you control), and the environment is the game world (the levels, obstacles, and everything around you).
                    You can see things happening in the game (like a monster coming toward you) and decide what to do next (maybe jump, run away, or fight back), which is the <b>environment</b>.
                </p>
                <h4>States and Observations</h4>
                <p>The <b>state</b> is like a snapshot of everything in the game world at a specific moment. For example, where the monsters are, what items are around, and how much health you have.
                    <b>Observations</b> are what the agent can see or know about the world. Sometimes you can see everything (like in a game where you can see the whole map), and sometimes you can only see part of it (like if you're inside a building in the game and can't see the outside).
                    This means the state is the complete picture (fully observed if agent can see the state) while the agent can only see part of it (partially observed, agent can only see a partial observation), most of the time.
                </p>
                <h4>Action Space</h4>
                <p><b>Action space</b> is the set of all possible things you can do in the game. 
                    For example, you might have options like "jump", "run", or "attack". 
                    These are the actions the agent can choose from.
                </p>
                <p>In some environments, such as Atari games and Go, the agent operates within <b>discrete action spaces</b>, meaning there are a limited number of possible actions it can take. 
                    Other environments, like where the agent controls a robot in a physical world, have <b>continuous action spaces</b>. 
                    In continuous spaces, actions are real-valued vectors.</p>
                <h4>Policies</h4>
                <p>A <b>policy</b> is like a set of rules or a plan that tells the agent what to do based on what it sees. 
                    So, if you see a monster, your policy might tell you to jump, or if it is attacking you first, you fight back. 
                    If you see a treasure, your policy might tell you to collect it.
                </p>
                <p>Since the policy acts as the agent's brain, the terms "policy" and "agent" are often used interchangeably. For example, one might say, "The policy aims to maximise the reward."</p>
                <p>In deep RL, we use parameterised policies. These policies are functions whose outputs depend on a set of parameters, such as the weights and biases of a neural network. 
                    In adjusting these parameters through optimisation algorithms, we can change the behavior of the policy.</p>
                <p>A policy can be either deterministic or stochastic.</p>
                <!-- see if deter and sto are like exploit and explore -->
                <h4>Trajectories</h4>
                <p>A <b>trajectory</b> is a sequence of states and actions in the world, like the path the agent takes through the game. 
                    It's the series of actions the agent takes, starting from the beginning until the end. 
                    It's like the story of what you do in the game.
                </p>
                <p>Trajectories are also frequently called episodes or rollouts.</p>
                <h4>Rewards</h4>
                <p>The goal of Reinforcement Learning (RL) is for <i><u>the agent to get better by practicing and learning from what happens when it takes different actions</u></i>. 
                    The agent gets rewards for good actions and tries to do more of those to maximise its total reward.
                </p>
                <p>The reward function, written as 𝑅
                    , tells the agent how good or bad a certain action was. This depends on:
                    Where the agent was (the state it was in, like a level in a game),
                    What action it just took (like moving, jumping, or grabbing something),
                    Where it ended up (the next state, after the action).
                    So we could write it as:
                    𝑅=𝑅(𝑠,𝑎,𝑠′)
                    </p>
                <p>Sometimes, though, it's simpler to look only at where the agent is now (or where it is and what action it took).</p>
                <p>The agent doesn’t just care about the reward from one action—it cares about getting the most points or rewards over time.
                    This total score over a period is called the return, written as R(τ). There are a couple of ways to look at return:
                    <ul>
                        <li><b>Finite-Horizon Undiscounted Return</b>: adding up all the points you get in a short level or time period in a game.</li>
                        <li><b>Infinite-Horizon Discounted Return</b>: look at all rewards the agent ever gets (an infinite horizon), but we use a discount factor (written as γ) to make future rewards worth a bit less.</li>
                    </ul>
                </p>
                <p>Prioritise short-term gains (like going for treasures) but still consider discounted long-term rewards (by factoring in future treasures after defeating the monster). Plus, using a discount factor makes the math easier. 
                    For each decision, it might calculate a value estimate based on both immediate rewards and discounted future rewards. 
                    So, even if it avoids one monster initially for a quick treasure, it might tackle the next if it seems worthwhile.</p>
            </div>
        </div>

        <h3>Initial Setup</h3>
        <p><span class="note">Note: This blog follows the instructions from <a href="https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html">this tutorial</a>. 
            Discussion and modifications that entails will be attempted to make sense of the article and the code.</span></p>
        <p>Setting up the environment was quite the adventure. 
            If you've worked with Python packages before, you know the usual suspects - version conflicts, deprecation warnings, and the occasional "this doesn't work like it used to" moments. 
        </p>
        
        <ul>
            <li>Outdated components</li>
            <li>Compatibility issues because some functions are deprecated</li>
            <li>Getting the right combination of dependencies' versions</li>
        </ul>
        <p>After a few hours of debugging and package juggling, I finally got everything working together.</p>

        <p>To list the whole dependencies I used would be a bit much, but here are the main ones:</p>
        <div class="code-window">
            <div class="console">
                <div class="red"></div>
                <div class="orange"></div>
                <div class="green"></div>
            </div>
        <pre class="code-content"><code>
pytorch=2.4.1=py3.8_cuda12.4_cudnn9_0
torchrl=0.5.0
torchvision=0.20.0
gym=0.26.1
gym-super-mario-bros=7.4.0
numpy=1.24.4
matplotlib-base=3.7.3
        </code></pre>
        </div>

        <p>You can see that I used CUDA and cuDNN for GPU acceleration.
            To find the right versions, I recommend checking <a href="https://stackoverflow.com/a/55717476">here</a>, <a href="https://pytorch.org/get-started/locally/">the official PyTorch website</a>, or using miniconda conda-forge channel for up-to-date packages.
            Once you have the right versions, it will look something like this:
        </p>
        <img src="mario-images/cuda.png" alt="CUDA device confirmation" style="display: block; margin-left: auto; margin-right: auto;">

        <h3>Initialise Environment</h3>
        <p>In Mario, tubes, mushrooms, etc... are the components of the environment. 
            This is where the agent will interact with the game world, taking actions and receiving rewards based on its performance.
        </p>

        <div class="code-window">
            <div class="console">
                <div class="red"></div>
                <div class="orange"></div>
                <div class="green"></div>
            </div>
        <pre class="code-content"><code>
# Initialise Super Mario environment (in v0.26 change render mode to 'human' to see results on the screen)
if gym.__version__ < '0.26':
    env = gym_super_mario_bros.make("SuperMarioBros-1-1-v0", new_step_api=True)
else:
    env = gym_super_mario_bros.make("SuperMarioBros-1-1-v0", render_mode='rgb', apply_api_compatibility=True)

# Define the movement options for Super Mario
MOVEMENT_OPTIONS = [
    ["right"],        # Move right
    ["right", "A"],   # Jump right
]

# Apply the wrapper to the environment
env = JoypadSpace(env, MOVEMENT_OPTIONS)

env.reset()
next_state, reward, done, trunc, info = env.step(action=0)
print(f"{next_state.shape},\n {reward},\n {done},\n {info}")
        </code></pre>
        </div>

        <p>This code snippet initialises the Super Mario environment, sets up the movement options, and applies a wrapper to the environment, which will helps the agent interact with the environment by providing a set of actions it can take.
            As a test, the code prints out the next state, reward, done flag, and info after taking the first action in the environment.
        </p>

        <div class="output-cell">
            <div class="output-line">(240, 256, 3), </div>
            <div class="output-line">0.0, </div>
            <div class="output-line">False, </div>
            <div class="output-line">{'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}</div>
        </div>
        
        <p>When you call env.step(action=0), you're telling Mario to perform action 0, which from the MOVEMENT_OPTIONS list is moving right. Of course, you can change the action to 1, which is a jump right.
            The function returns the next state (a 240x256x3 image), the reward (0.0 in this case), a done flag (False, meaning the episode is not over), and some additional information about the environment.
            This is something that Mario will probably see (for illustration purposes):
        </p>
        <img src="mario-images/mario-screen.svg" alt="Mario screen" style="display: block; margin-left: auto; margin-right: auto; width: 45%;">
        <p>Think of it like pressing the right button for a split second, checks if you get a reward, game over or not, and then returns the new state of the game.
            This is a single step in the game, the interaction loop will continue until the game is over or the AI beat the game.
        </p>

        <h3>Pre-process the Environment</h3>

        <p>In the previous output, the next state was a 240x256x3 image which is returned by the environment.
            Often, this is too much information for the agent to process directly.
            Mario does not need to see the entire screen to make decisions.
            Instead, we will apply wrappers to the environment to pre-process the images and make them more manageable for the agent.
        </p>
        <img src="mario-images/env-wrapper.png" alt="Environment wrapper" style="display: block; margin-left: auto; margin-right: auto;">
        <img src="mario-images/frame-transformation.svg" alt="Frame transformation" style="display: block; margin-left: auto; margin-right: auto; width: 70%;">

        <p>The final output is smaller by almost 85%, which means faster processing and less memory usage.
            This is much simpler than what a human sees, but contains all the essential information needed to learn how to play the game effectively.           
        </p>

        <h3>Replay Buffer</h3>

        <p>A replay buffer is like a "memory bank" that stores the agent's experiences while it plays the game. Each experience consists of:
            The current state (what Mario sees),
            the action taken,
            the reward received,
            the next state,
            and whether the game ended (done flag). Something like this:
        </p>
        <img src="mario-images/replay-buffer-structure.svg" alt="Replay buffer structure" style="display: block; margin-left: auto; margin-right: auto; width: 70%;">

        <p>In <a href="https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html">this tutorial</a>, the author uses <b>TensorDictReplayBuffer with LazyMemmapStorage</b>, which is a custom replay buffer implementation.
            The replay buffer stores experiences (state, action, reward, next state, done) and samples mini-batches for training the agent.
            Only, I failed to get it running.
        </p>
        <div class="output-cell">
            <div class="output-line">OSError: [WinError 1455] The paging file is too small for this operation to complete</div>
        </div>
        <p>Apparently, this is a Windows-specific error occurs when the system tries to allocate more virtual memory than is available.
            Since I don't want to mess with the system too much and I don't have any money for RAM yet, I decided to use a simpler replay buffer implementation and called it SimpleReplayBuffer.
        </p>

        <table>
            <thead>
                <tr>
                    <th>SimpleReplayBuffer</th>
                    <th>TensorDictReplayBuffer with LazyMemmapStorage</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Uses simple Python deque for storage</td>
                    <td>More sophisticated memory management</td>
                </tr>
                <tr>
                    <td>Implements basic prioritised experience replay</td>
                    <td>Uses memory mapping for large datasets</td>
                </tr>
                <tr>
                    <td>Straightforward memory management</td>
                    <td>More complex data structures</td>
                </tr>
                <tr>
                    <td>Less feature-rich but more robust</td>
                    <td>More features but more potential points of failure</td>
                </tr>
            </tbody>
        </table>
        <img src="mario-images/memory-usage.svg" alt="Memory usage comparison" style="display: block; margin-left: auto; margin-right: auto; width: 80%;">
        <p>My <b>SimpleReplayBuffer</b> does 2 key things:</p>
        <ol>
            <li>push (Storing Experiences): 
                <ul>
                    <li>It takes a snapshot of what happened during one step of Mario's gameplay.</li>
                    <li>It calculates how important this memory is (priority = |reward| +  epsilon).</li>
                    <li>It stores the experience in a deque and the priority in a separate deque.</li>
                </ul>
            </li>
            <li>sample (Prioritised Sampling): 
                <ul>
                    <li>Think of this like Mario "remembering" past experiences to learn from. He will takes in how many memories to recall (batch_size) and prefers to remember important moments (high rewards).</li>
                    <li>If picking importance memories failed (due to zero probabilities), it will fallback to random memories.</li>
                    <li>Then, it converts chosen memories to format for learning (GPU tensors).</li>
                </ul>
            </li>
        </ol>
        <p>Even though it is essentially the same with TensorDictReplayBuffer, the <b>SimpleReplayBuffer</b> is actually a better solution for my current setup. 
            It is lightweight, easy to understand, and doesn't require any special dependencies.
            Plus, it is easier to debug and modify if needed.
        </p>

        <h3>It's-a me, Mario!</h3>
        <p>It's time create the Mario-playing agent. Basically, Mario should be able to:
        </p>
        <ul>
            <li><b>Act</b> according to the optimal policy (exploit) or try new things (explore) based on the current state.</li>
            <li><b>Remember</b> experiences. Experience = (current state, current action, reward, next state). 
                Mario will <b>cache</b> and <b>recall</b> these experiences to update his policy.</li>
            <li><b>Learn</b> from these experiences, which means updating his policy to maximise rewards over time.</li>
        </ul>
        <img src="mario-images/mario-agent-overview.svg" alt="Mario agent overview" style="display: block; margin-left: auto; margin-right: auto;">

        <div class="dropdown">
            <h3 class="dropdown-heading" onclick="toggleDropdown(this)">Act</h3>
            <div class="dropdown-content">
                <div class="code-window">
                    <div class="console">
                        <div class="red"></div>
                        <div class="orange"></div>
                        <div class="green"></div>
                    </div>
                <pre class="code-content"><code>
def act(self, state):
    """
    Given a state, choose an epsilon-greedy action and update value of step.

    Inputs:
    state(``LazyFrame``): A single observation of the current state, dimension is (state_dim)
    Outputs:
    ``action_idx`` (``int``): An integer representing which action Mario will perform
    """
                </code></pre>
                </div>

                <p>The act function takes a state, which is a LazyFrame object representing what Mario "sees" at a given moment.
                    From there, it chooses an action based on an epsilon-greedy strategy (exploit vs. explore).
                    If the agent is exploring (randomly trying new things), it will choose a random action.
                    If the agent is exploiting (using what it knows to maximise rewards), it will choose the best action by relying on MarioNet in the Learn section.
                </p>

                <img src="mario-images/act-method-flow.svg" alt="Act method flow" style="display: block; margin-left: auto; margin-right: auto; width: 70%;">
                <p>The method is called every time Mario needs to make a decision, which happens many times per second during gameplay.
                </p>

                <p>Exploration rate early on is 1, meaning Mario will explore 100% of the time early in training.
                    Over time, this rate will decrease through <code>exploration_rate_decay</code>, so Mario will explore less and force to exploit more.
                    This value will never goes below <code>exploration_rate_min</code>, which can be set.
                </p>
            </div>
        </div>
        
        <div class="dropdown">
            <h3 class="dropdown-heading" onclick="toggleDropdown(this)">Remember (Cache and Recall)</h3>
            <div class="dropdown-content">
                <div class="code-window">
                    <div class="console">
                        <div class="red"></div>
                        <div class="orange"></div>
                        <div class="green"></div>
                    </div>
                <pre class="code-content"><code>
def cache(self, state, next_state, action, reward, done):
    """
    Store the experience to self.memory (replay buffer)

    Inputs:
    state (``LazyFrame``),
    next_state (``LazyFrame``),
    action (``int``),
    reward (``float``),
    done(``bool``)
    """

def recall(self):
    """
    Retrieve a batch of experiences from memory
    """
                </code></pre>
                </div>

                <p>These 2 functions serve as the memory bank for Mario.</p>
                <p>The cache function stores experiences (state, next state, action, reward, done) in the memory (replay buffer).
                    The recall function randomly retrieves a batch of experiences from the replay buffer for training.
                    This is where Mario "remembers" past experiences and uses them to learn.
                </p>

                <p>There are 2 keys parameters to consider when caching and recalling experiences: <b>the buffer size</b> and <b>the batch size</b>.</p>
                <img src="mario-images/mario-memory-flow.svg" alt="Mario memory flow" style="display: block; margin-left: auto; margin-right: auto; width: 85%;">
                <p><b>The buffer size</b> determines how many experiences Mario can remember at once.
                    For Mario, each level lasts around 200-300 seconds. 50000 experiences should be enough to cover 150-250 full levels, making it a good starting point.
                    If you have the computational resources, you can increase this number to remember more experiences -> diverse learning, important events are not forgotten.</p>
                <p><b>The batch size</b> determines how many experiences Mario will recall for training.
                    A larger batch size means Mario can learn from more experiences at once, which can lead to more stable learning.
                    However, larger batch sizes require more memory and computational resources, so it's a trade-off.
                </p>
            </div>
        </div>

        <div class="dropdown">
            <h3 class="dropdown-heading" onclick="toggleDropdown(this)">Learn</h3>
            <div class="dropdown-content">
                <h4>MarioNet</h4>
                <div class="code-window">
                    <div class="console">
                        <div class="red"></div>
                        <div class="orange"></div>
                        <div class="green"></div>
                    </div>
                <pre class="code-content"><code>
class MarioNet(nn.Module):
    """mini CNN structure
    input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output
    """
                </code></pre>
                </div>

                <p>The MarioNet class defines the neural network architecture that Mario uses to learn how to play the game.
                    In this instance, we use Double Deep-Q Network. For clarity, the blog won't drill into the details.
                    However, know that DDQN can provide a more stable training, better performance, and relatively simple to implement compare to DQN or Dueling DQN which is way more complex.
                </p>

                <h4>TD Estimate and Target (Temporal Difference Learning)</h4>
                <div class="code-window">
                    <div class="console">
                        <div class="red"></div>
                        <div class="orange"></div>
                        <div class="green"></div>
                    </div>
                <pre class="code-content"><code>
@torch.cuda.amp.autocast()
def td_estimate(self, state, action):
    """Compute current Q-value estimate"""

@torch.cuda.amp.autocast()
@torch.no_grad()  # No gradients needed for target
def td_target(self, reward, next_state, done):
    """Compute TD target using DDQN"""
                </code></pre>
                </div>

                <p>In short, TD learning is about estimating the value of a state-action pair and updating this estimate based on the reward and the expected value of the next state.</p>
                <ol>
                    <li><b>TD Estimate</b> (Current Guess):</li>
                        <ul>
                            <li>What Mario thinks the value of current state-action pair is</li>
                            <li>Comes from online network (current knowledge)</li>
                            <li>Like Mario saying "I think jumping here will give me X points"</li>
                        </ul>
                    <li><b>TD Target</b> (Better Guess):</li>
                        <ul>
                            <li>What Mario should think the value is, based on immediate reward received and estimated future value from more stable target network.</li>
                            <li>Comes from target network (more accurate)</li>
                            <li>Like a teacher saying "Actually, jumping there gave you Y points plus future possibilities"</li>
                        </ul>
                </ol>
                <img src="mario-images/td-learning-simple.svg" alt="TD learning" style="display: block; margin-left: auto; margin-right: auto; width: 70%;">
                <p>Over time, learning happens by minimising difference and Mario's estimates get closer to reality.
                </p>

                <h4>Model Updates (Training Process)</h4>
                <div class="code-window">
                    <div class="console">
                        <div class="red"></div>
                        <div class="orange"></div>
                        <div class="green"></div>
                    </div>
                <pre class="code-content"><code>
def update_Q_online(self, td_estimate, td_target):
    loss = self.loss_fn(td_estimate, td_target)  # How wrong was Mario?
    self.optimizer.zero_grad()                   # Clear previous gradients
    loss.backward()                              # Calculate gradients
    self.optimizer.step()                        # Update network weights
    return loss.item()

def sync_Q_target(self):
    # Copy online network weights to target network
    self.net.target.load_state_dict(self.net.online.state_dict())
                </code></pre>
                </div>

                <p>These functions are responsible for updating the online network based on the TD estimate and TD target.
                    It's like Mario's brain adjusting its understanding
                    The update_Q_online function calculates the loss between the TD estimate and TD target, backpropagates the loss to update the network weights, and returns the loss value.
                    The sync_Q_target function, which will be called occasionally, synchronises the target network with the online network by copying the weights so that the target network is updated with the latest knowledge.
                </p>

                <h4>Learning Loop</h4>
                <div class="code-window">
                    <div class="console">
                        <div class="red"></div>
                        <div class="orange"></div>
                        <div class="green"></div>
                    </div>
                <pre class="code-content"><code>
def learn(self):
    # Important timings
    if self.curr_step < self.burnin:         # First 5000 steps
        return None, None                    # Just collect experiences
        
    if self.curr_step % self.learn_every != 0:   # Every 2 steps
        return None, None                        # Skip learning
        
    if self.curr_step % self.sync_every == 0:    # Every 5000 steps
        self.sync_Q_target()                     # Sync target network

    if self.curr_step % self.save_every == 0:    # Every 500,000 steps
        self.save()                              # Save checkpoints

    # Main learning steps
    state, next_state, action, reward, done = self.recall()  # Get batch
    td_est = self.td_estimate(state, action)                 # Current guess
    td_tgt = self.td_target(reward, next_state, done)        # Better guess
    loss = self.update_Q_online(td_est, td_tgt)              # Learn
                </code></pre>
                </div>

                <p>The learn function is the heart of the agent's learning process.
                    It decides when to start learning, when to skip learning, and when to synchronise the target network.
                    <ul>
                        <li>First 5000 steps: Just collect experiences</li>
                        <li>Every 2 steps: Skip learning</li>
                        <li>Every 5000 steps: Sync target network</li>
                        <li>Every 500,000 steps: Save checkpoint</li>
                    </ul>
                </p>
                <p>The flow would be something like this: Get Batch → Calculate Guess → Calculate Target → Update Network</p>
            </div>
        </div>

        <h3>Logging</h3>
        <p>For tracking and visualising the training process, the MetricLogger class is created.
            It is a comprehensive logging system used to track, and save, and visualise the agent's performance during training.
        </p>
        <img src="mario-images/metric-logger-diagram.svg" alt="Metric logger diagram" style="display: block; margin-left: auto; margin-right: auto;">
        <p>In essence, these metrics help understand the agent's learning:</p>
        <ul>
            <li>Increasing rewards indicate the agent is getting better</li>
            <li>Decreasing losses show stable learning, that the agent is learning from its mistakes</li>
            <li>Increasing Q-values show the agent is finding better strategies</li>
            <li>Episode lengths can indicate if Mario is surviving longer or completing levels faster</li>
        </ul>

        <h3>Evaluation</h3>
        <p>I have made a evaluation script in order to watch MarioNet plays.
            Because after all, the goal is to see Mario beat the game by itself, right?
        </p>
        <img src="mario-images/mario-eval-flow.svg" alt="Mario evaluation flow" style="display: block; margin-left: auto; margin-right: auto; width: 80%;">
        
        <p>The script runs in human-visible mode (render_mode="human") and records video, 
            making it easy to analyse the agent's behavior.
        </p>
        
        <p>The evaluation process is somewhat similar to the training process.
            The eval script still initiate environment setup and MarioNet, but it will load the checkpoint provided by the training process earlier.
            Then, we implement gameplay loop, run the Mario session played by the model, and record its performance.
            This is where you can see how well Mario is performing and if he's improving over time.
        </p>

        <div class="video-container">
            <video controls>
                <source src="../projects/mario/mario_gameplay_20241117_135729.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <p class="video-caption">Mario Gameplay - Recorded on November 17, 2024</p>
        </div>

        <p>As you can see, he's stuck. But that's okay, it's part of the learning process.
            The agent will learn from these experiences and improve over time.
        </p>

        <p>The script will track several metrics to demonstrate its playing styles and identify areas for improvement:
            Total steps taken, final x-position reached, chosen actions, whether Mario reached the flag, and average speed (pixels/step).
        </p>

        <h3>Training Process and Parameter Optimisation</h3>
        <p>Training a reinforcement learning agent to play Super Mario Bros is computationally intensive. 
            With my RTX 3060 Ti 8GB GPU setup, I had to make several compromises and optimisations to make the training feasible. 
            Here's what I learned from two different training approaches.
        </p>

      <table>
        <thead>
            <tr>
                <th>Training Approach</th>
                <th>Results</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><b>First Training Run</b>: Fast but Limited</td>
                <td>
                    <ul>
                        <li>Duration: 1-2 hours</li>
                        <li>Episodes: 1000</li>
                        <li>Action Space: 2 actions (only jump right and move right)</li>
                        <li>Parameters: More aggressive learning rates and exploration decay</li>
                    </ul>
                </td>
            </tr>
            <tr>
                <td><b>Second Training Run</b>: Longer but More Complex</td>
                <td>
                    <ul>
                        <li>Duration: 10-11 hours</li>
                        <li>Episodes: 5000</li>
                        <li>Action Space: 6 actions (no move, right, run right, jump right, run+jump right, charge)</li>
                        <li>Parameters: More conservative settings for stability</li>
                    </ul>
                </td>
            </tr>
        </tbody>
      </table>

      <div class="code-window">
        <div class="console">
            <div class="red"></div>
            <div class="orange"></div>
            <div class="green"></div>
        </div>
    <pre class="code-content"><code>
# First Run (Aggressive)
batch_size = 32                      # Smaller batch size for faster learning
exploration_rate_decay = 0.99999975  # Slower decay for more exploration
gamma = 0.90                         # Higher discount factor for long-term rewards
learning_rate = 0.00025              # Faster or slower learning
SimpleReplayBuffer(100000, ...)      # Larger buffer for more experiences

# Second Run (Conservative)
batch_size = 256                     # Larger batch size for more stable learning
exploration_rate_decay = 0.99999
gamma = 0.95 
learning_rate = 0.0005
SimpleReplayBuffer(50000, ...)       # Smaller buffer for less memory usage
    </code></pre>
    </div>

        <p>After messing around, here are what I conclude:
        </p>

        <ul>
            <li><b>Action Space Trade-off</b>: The 2-action model (first run) performed well, if not matching the 6-action model. 
                This suggests that for limited computational resources, a simpler action space might be more efficient.</li>
            <li><b>Behavioral Patterns</b>: To be observed, since both models occasionally stuck in-place, indicating incomplete learning.</li>
            <li><b>Time</b>: The 2-action model is so much faster. 
                The 6-action model, while showed more diverse strategies, required significantly more training time to be optimal.</li>
        </ul>

        <p>Certainly, I will continue to experiment with different hyperparameters and training strategies to improve the agent's performance.
            What I have in mind at first is using the 2-action model and train it in smaller episodes.
            I will also try to explore extreme parameters to force the model to converge faster, since hardware is limited in a way.
        </p>

        <p>I will update with more findings in part 2 soon enough. 
            In the mean time, you can check out <a href="../projects/mario/mario.html">this project in a concise manner</a> or <a href="../blog.html">my other projects</a>.
        </p>
        </div>

        <section id="references">
        <h2>References</h2>
        <ol>
            <li>
            Van Hasselt, Hado & Guez, Arthur & Silver, David. (2015). Deep Reinforcement Learning with Double Q-Learning. Proceedings of the AAAI Conference on Artificial Intelligence. 30. <a target="_blank" href="http://arxiv.org/pdf/1509.06461.pdf">http://arxiv.org/pdf/1509.06461.pdf</a>
            </li>
            <li>
            Feng, Yuansong & Subramanian, Suraj & Wang, Howard & Guo, Steven. Train a Mario-playing RL Agent. Available at: <a target="_blank" href="https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html">https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html</a>
            </li>
            <li>
            OpenAI Spinning Up tutorial: <a target="_blank" href="https://spinningup.openai.com/en/latest/">https://spinningup.openai.com/en/latest/</a>
        </ol>
        </section>
          
    <p><a href="index.html">Back to Home</a></p>

    <footer>
        <hr>
        <p>Made by Ethan, 2024, with 💜. <a href="mailto:phatnguyenground@gmail.com">Email me</a></p>
    </footer>
</div>

</body>
<script src="../js/scripts.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>
<script>
// JavaScript to set the date modified
    document.getElementById('date-modified').textContent = document.lastModified;
    function toggleDropdown(element) {
    element.parentElement.classList.toggle("open");
  };
</script>
</html>    