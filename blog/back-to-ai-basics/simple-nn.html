<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>My Blogs - Back to AI Basics - Simple Neural Network</title>
    <meta name="description" content="A simple neural network implementation for beginners.">
    <link rel="stylesheet" href="../../css/blog-styles.css">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <script>
        // Immediately apply the saved theme
        (function() {
            const savedTheme = localStorage.getItem('theme') || 'light';
            document.documentElement.setAttribute('data-theme', savedTheme);
        })();
    </script>
    <style>
        .button {
            display: inline-block;
            padding: 10px 20px;
            margin-top: 10px;
            background-color: var(--accent-color);
            color: var(--bg-primary);
            text-decoration: none;
            border-radius: 5px;
            transition: background-color 0.3s;
        }

        .button:hover {
            background-color: #0073e6;
        }
        .dropdown-heading {
            cursor: pointer;
            font-weight: bold;
            padding: 10px;
            padding-left: 30px; /* Add space for triangle on the left */
            border: 1px solid var(--border-color);
            background-color: var(--bg-secondary);
            margin: 0;
            position: relative;
        }

        /* Triangle icon using ::before pseudo-element */
        .dropdown-heading::before {
            content: '';
            border-style: solid;
            border-width: 5px 5px 0 5px; /* Triangle pointing down */
            border-color: var(--text-primary) transparent transparent transparent;
            position: absolute;
            left: 10px; /* Position triangle on the left */
            top: 50%;
            transform: translateY(-50%);
            transition: transform 0.3s; /* Smooth rotation */
        }

        .dropdown-content {
            display: none;
            padding: 10px;
            border: 1px solid #ddd;
            border-top: none;
        }

        /* Show content and rotate triangle when open */
        .dropdown.open .dropdown-content {
            display: block;
        }

        /* Rotate the triangle to point up */
        .dropdown.open .dropdown-heading::before {
            transform: translateY(-50%) rotate(180deg);
        }
    </style>
</head>

<body>
    <aside class="sidebar">
        <div class="header-content">
            <img id="logo" src="../../images/logo2.webp" alt="Logo">
            <div class="text-content">
                <h1>Hi! I'm Ethan</h1>
                <h3>Hard learning. Hardworking. Hard playing. Ready for what's next.</h3>
            </div>
        </div>
        <div class="theme-toggle">
            <button class="theme-btn" id="themeToggle" aria-label="Toggle theme">
                <i class="fa-solid fa-sun"></i>
                <i class="fa-solid fa-moon"></i>
            </button>
        </div>
        <nav id="nav-bar">
            <a href="../../index.html">Homepage</a>
            <a href="../../portfolio.html">My Portfolio</a>
            <a href="../../blog.html" class="current">My Blogs</a>
        </nav>
        <div class="sidebar-extra">
            <p><a href="mailto:phatnguyenground@gmail.com"><i class="fa-solid fa-at"></i></a>
                <a href="https://github.com/ethannguyen2k" target="_blank"><i class="fa-brands fa-github-alt"></i></a>
                <a href="https://linkedin.com/in/ethan-2k" target="_blank"><i class="fab fa-linkedin"></i></a></p>
        </div>
    </aside>

    <div class="container2 fade-in">
        <div class="blog-content">
            <h2>Back to AI Basics: Simple Neural Network</h2>
            <div class="blog-info">
                <p>Date Created: <span id="date-created">May 15, 2025</span></p>
                <p>Date Modified: <span id="date-modified"></span></p>
            </div>            
            <a target="_blank" href="https://github.com/ethannguyen2k/back-to-AI-basics/blob/main/models/simplenn.ipynb" class="button">Code for this topic on GitHub</a>

            <p>You can think of this as step 0 of the series. In this step, we will build a simple neural network from scratch using Python and NumPy. 
                This will help you understand the basic concepts of neural networks before diving into more complex architectures.</p>

            <p>I will mainly talk about the math underneath. 
                Hopefully, when you done with this blog, you will understand what is happenning under the hood:</p>
            <ul>
                <li>Matrix multiplications (np.dot)</li>
                <li>Simple activation functions (ReLU, softmax)</li>
                <li>Basic math operations</li>
            </ul>

            <p>If you want to understand layers in neural networks in a more practical way,
                you can check out my <a href="mlp-mnist.html" target="_blank">MLP on MNIST</a> blog. 
                I will show you how to build a simple MLP using PyTorch and train it on the MNIST dataset.</p>

            <p>Or if you want to know more about optimizers, loss functions, etc.,
                check out my <a href="https://github.com/ethannguyen2k/back-to-AI-basics" target="_blank">GitHub repo</a> for this series.
            </p>

            <h3>The Working</h3>
            <p>A neural network works by mimicking how the human brain processes information.</p>
            <p>It consists of multiple layers of interconnected nodes (neurons), not just one, where each node takes input, applies weights and a bias, runs it through an activation function, and passes the result to the next layer.</p>
            <p>During training, the network adjusts its weights using backpropagation to minimize the error between its predictions and the actual values. 
                Over time, it learns to recognize patterns and make accurate predictions or classifications based on the input data.</p>

            <p>In this blog, I will show you how to implement a simple neural network from scratch using Python and NumPy.</p>
            
            <h3>The Model</h3>

            <p>Our model will be Input Layer (2 neurons) → Hidden Layer (3 neurons with ReLU) → Output Layer (2 neurons with Softmax)</p>
            <img src="../back-to-ai-basics-images/simplenn_architecture.svg" alt="Simple Neural Network Architecture" class="blog-image">
            <p>This is a basic Multilayer Perceptron (MLP) (<a href="mlp-mnist.html#mlp">I talked more about MLP here</a>). 
                It is typically refers to fully connected feedforward neural networks with at least one hidden layer, just like this one.
                I will follow closely with my notebook for this blog post. You can view it here.</p>
            <a target="_blank" href="https://github.com/ethannguyen2k/back-to-AI-basics/blob/main/models/simplenn.ipynb" class="button">Code for this topic on GitHub</a>
            
            <h3>The Working</h3>
            <h4>The Input</h4>
            <p>We will use a simple dataset of 200 samples with 2 features and 2 classes. 
                The dataset is generated using NumPy's random number generator.</p>

            <p>The input layer has 2 neurons that take the 2 features of the dataset. 
            </p>
            <h4>The Hidden Layer</h4>
            <p>The hidden layer has 3 neurons that apply <a href="https://github.com/ethannguyen2k/back-to-AI-basics/blob/main/active_functions.ipynb" target="_blank">the ReLU activation function</a>.</p>
            <p>The input data is passed through the hidden layer. 
                Each neuron in the hidden layer takes the input from the input layer, applies weights and a bias, and runs it through the ReLU activation function.
            </p>
            <div class="code-window">
                <div class="console">
                    <div class="red"></div>
                    <div class="orange"></div>
                    <div class="green"></div>
                </div>
                <pre class="code-content">
output = activation(weights · inputs + bias)

or in this case:

a = ReLU(w · x + b)
                </pre>
            </div>

            <p>So for example:</p>
            <table class="table-common">
                <thead>
                    <tr>
                    <th>Layer</th>
                    <th>Description</th>
                    <th>Shape</th>
                    <th>Values</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                    <td>Input → Hidden Weights \(W_1\)</td>
                    <td>Weights connecting input to hidden</td>
                    <td>(2, 3)</td>
                    <td>[[-0.6, 0.6, 0.4], [-0.6, 0.7, 0.5]]</td>
                    </tr>
                    <tr>
                    <td>Hidden Biases \(b_1\)</td>
                    <td>Biases of hidden layer</td>
                    <td>(1, 3)</td>
                    <td>[[0.3, 0.5, 0.3]]</td>
                    </tr>
                    <tr>
                    <td>Hidden → Output Weights \(W_2\)</td>
                    <td>Weights connecting hidden to output</td>
                    <td>(3, 2)</td>
                    <td>[[-0.7, 0.7], [0.8, -0.8], [0.5, -0.5]]</td>
                    </tr>
                    <tr>
                    <td>Output Biases \(b_2\)</td>
                    <td>Biases of output layer</td>
                    <td>(1, 2)</td>
                    <td>[[-0.4, 0.4]]</td>
                    </tr>
                </tbody>
                </table>

            <p>Let's say we have [0.5, 0.5] as the input.</p>
            <h4>Step 1: Input → Hidden Layer</h4>
            <p>Compute hidden layer pre-activation:</p>
            
            <p>
            \[
            z_1 = x \cdot W_1 + b_1
            = \begin{bmatrix}0.5, 0.5\end{bmatrix}
            \cdot \begin{bmatrix}-0.6, 0.6, 0.4\\-0.6, 0.7, 0.5\end{bmatrix} + \begin{bmatrix}0.3, 0.5, 0.3\end{bmatrix}
            = \begin{bmatrix}-0.3, 1.15, 0.75\end{bmatrix}
            \]
            </p>

            <h4>Step 2: Apply ReLU activation</h4>
            <p>Apply ReLU activation function:</p>

            <p>
            \[
            a_1 = \text{ReLU}(z_1) = \text{ReLU}(\begin{bmatrix}-0.3, 1.15, 0.75\end{bmatrix}) = \begin{bmatrix}0, 1.15, 0.75\end{bmatrix}
            \]
            </p>

            <h4>Step 3: Hidden → Output Layer</h4>
            <p>Compute output layer pre-activation:</p>

            <p>
            \[
            z_2 = a_1 \cdot W_2 + b_2
            = \begin{bmatrix}0, 1.15, 0.75\end{bmatrix}
            \cdot \begin{bmatrix}-0.7, 0.7\\0.8, -0.8\\0.5, -0.5\end{bmatrix} + \begin{bmatrix}-0.4, 0.4\end{bmatrix}
            = \begin{bmatrix}0.895, -0.895\end{bmatrix}
            \]
            </p>

            <h4>Step 4: Apply Softmax activation</h4>
            <p>Softmax formula:</p>

            <p>
            \[
            \text{softmax}(z) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
            \]
            </p>

            <p>First, I'll compute \( e^{z_i} \) for each output neuron:</p>
            <p> \( e^{0.895} \approx 2.447 \) and \( e^{-0.895} \approx 0.408 \)</p>
            <p>Then, I'll compute the sum of these values:</p>
            <p> \( \sum_{j=1}^{K} e^{z_j} \ = 2.447 + 0.408 \approx 2.855 \)</p>
            <p>Finally, I'll divide each \( e^{z_i} \) by this sum:</p>
            <p>
            \[
            a_2 = \text{softmax}(z_2) = \begin{bmatrix}\frac{2.447}{2.855}, \frac{0.408}{2.855}\end{bmatrix} \approx \begin{bmatrix}0.857, 0.143\end{bmatrix}
            \]
            </p>

            <p>We can verify that these values sum to 1: 0.857 + 0.143 = 1</p>

            <p>In the end,
                this transformation has converted our original values into a probability distribution, 
                where the first class has about an 85.69% probability and the second class has about a 14.31% probability.
            </p>

            <p>And that's just one forward pass through the network!
                Imagine doing this for thousands of samples in a dataset.
            </p>

            <h4>Step 5: Backpropagation</h4>
            <p>Backpropagation is the process of updating the weights and biases of the network based on the error between the predicted output and the actual output.</p>
            <p>Let's go back to the previous result, \( \begin{bmatrix}0.857, 0.143\end{bmatrix} \)</p>
            <p>Let's say the true label is class 1, and you're indexing from 0 (i.e., class 0 and class 1), then the one-hot label is: \( \begin{bmatrix}0, 1\end{bmatrix} \)</p>
            <p>Usually, this would indicate that the model is not performing well since it's assigning a high probability to the wrong class.</p>
            <p>We have some common loss functions, such as MSE for regression. For this one, we will be using cross-entropy for classification.</p>
            <p>Cross-entropy loss function:</p>

            <p>
            \[
            L(y, \hat{y}) = -\sum_{i=1}^{C} y_i \log(\hat{y}_i)
            \]
            </p>

            <p>Where \( y \) is the true label, \( \hat{y} \) is the predicted label, and \( C \) is the number of classes.</p>
            <p>Since only \( y_1 \) is 1, the rest are 0:</p>

            <p>
            \[
            L(y, \hat{y}) = -\sum_{i=1}^{C} y_i \log(\hat{y}_i) = -y_0 \log(\hat{y}_0) - y_1 \log(\hat{y}_1) = -0 \cdot \log(0.857) - 1 \cdot \log(0.143) \approx 1.944
            \]
            </p>

            <p>Now, we need to compute the gradients of the loss function with respect to the weights and biases.</p>
            <p>It can be a bit too much since we are using 2 different activation functions, so buckle up.</p>
            <p>First, let's denote again:</p>
            <ul>
                <li>Input: \( x \)</li>
                <li>Weights and biases of the hidden layer: \( W_1, b_1 \)</li>
                <li>Weights and biases of the output layer: \( W_2, b_2 \)</li>
                <li>Pre-activation of the hidden layer: \( z_1 = W_1 \cdot x + b_1 = \begin{bmatrix}-0.3, 1.15, 0.75\end{bmatrix} \)</li>
                <li>Activation of the hidden layer: \( a_1 = \text{ReLU}(z_1) = \begin{bmatrix}0, 1.15, 0.75\end{bmatrix} \)</li>
                <li>Pre-activation of the output layer: \( z_2 = W_2 \cdot a_1 + b_2 = \begin{bmatrix}0.895, -0.895\end{bmatrix} \)</li>
                <li>Activation of the output layer: \( a_2 = \text{softmax}(z_2) = \begin{bmatrix}0.857, 0.143\end{bmatrix} \)</li>
                <li>Cross-entropy loss: \( L(y, \hat{y}) \approx 1.944 \)</li>
            </ul>

            <p>Let's calculate the gradients via backpropagation:</p>
            <p class="step">1. Compute the gradient of the loss with respect to the output layer activation:</p>
            <p>For cross-entropy loss with softmax output, if y is a one-hot encoded vector with the true class index, then:</p>

            <p>
            \[
            \frac{\partial L}{\partial a_2} = -\frac{y}{a_2}
            \]
            </p>

            <p class="step">2. Compute the gradient of the loss with respect to the pre-activation of the output layer:</p>
            <p>A key property of softmax combined with cross-entropy loss is that:</p>

            <p>
            \[
            \frac{\partial L}{\partial z_2} = a_2 - y
            \]
            </p>

            <p>This is a convenient simplification. So if \( y = \begin{bmatrix}0, 1\end{bmatrix} \) and \( a_2 = \begin{bmatrix}0.857, 0.143\end{bmatrix} \), then:</p>

            <p>
            \[
            \frac{\partial L}{\partial z_2} = a_2 - y = \begin{bmatrix}0.857, 0.143\end{bmatrix} - \begin{bmatrix}0, 1\end{bmatrix} = \begin{bmatrix}0.857, -0.857\end{bmatrix}
            \]
            </p>

            <p class="step">3. Compute the gradient of the loss with respect to the weights of the output layer:</p>
            <p>Using the chain rule:</p>

            <p>
            \[
            \frac{\partial L}{\partial W_2} = \frac{\partial L}{\partial z_2} \cdot \frac{\partial z_2}{\partial W_2} = \frac{\partial L}{\partial z_2} \cdot a_1^T
            \]
            </p>

            <p>Where \( a_1^T \) is the transpose of the activation of the hidden layer.</p>
            <p>So:</p>

            <p>
            \[
            \frac{\partial L}{\partial W_2} = \begin{bmatrix}0.857, -0.857\end{bmatrix} \cdot \begin{bmatrix}0, 1.15, 0.75\end{bmatrix}^T
            = \begin{bmatrix}0.857 \cdot 0, 0.857 \cdot 1.15, 0.857 \cdot 0.75\\-0.857 \cdot 0, -0.857 \cdot 1.15, -0.857 \cdot 0.75\end{bmatrix}
            \approx \begin{bmatrix}0, 0.986, 0.643\\0, -0.986, -0.643\end{bmatrix}
            \]
            </p>

            <p class="step">4. Compute the gradient of the loss with respect to the biases of the output layer:</p>
            <p>Similarly:</p>

            <p>
            \[
            \frac{\partial L}{\partial b_2} = \frac{\partial L}{\partial z_2} \cdot \frac{\partial z_2}{\partial b_2} = \frac{\partial L}{\partial z_2} \cdot 1
            = \frac{\partial L}{\partial z_2}
            = \begin{bmatrix}0.857, -0.857\end{bmatrix}
            \]
            </p>

            <p class="step">5. Compute the gradient of the loss with respect to the hidden layer activation:</p>

            <p>
            \[
            \frac{\partial L}{\partial a_1} = \frac{\partial L}{\partial z_2} \cdot \frac{\partial z_2}{\partial a_1} = \frac{\partial L}{\partial z_2} \cdot W_2^T
            = \begin{bmatrix}0.857, -0.857\end{bmatrix} \cdot \begin{bmatrix}-0.7, 0.8, 0.5\\0.7, -0.8, -0.5\end{bmatrix}
            \approx \begin{bmatrix}-1.2, 1.37, 0.857\end{bmatrix}
            \]
            </p>

            <p class="step">6. Compute the gradient of the loss with respect to the pre-activation of the hidden layer:</p>
            <p>For ReLU, the gradient is 1 for positive values and 0 for negative values:</p>

            <p>
            \[
            \frac{\partial L}{\partial z_1} = \frac{\partial L}{\partial a_1} \cdot \frac{\partial a_1}{\partial z_1} = \frac{\partial L}{\partial a_1} \cdot 1_{\{z_1 > 0\}}
            \]
            </p>
            <p>Where \( 1_{\{z_1 > 0\}} \) is an indicator function that is 1 if \( z_1 > 0 \) and 0 otherwise.</p>

            <p>So:</p>
            <p>
            \[
            \frac{\partial L}{\partial z_1} = \begin{bmatrix}-1.2, 1.37, 0.857\end{bmatrix} \cdot 1_{\{-0.3, 1.15, 0.75\}} = \begin{bmatrix}0, 1.37, 0.857\end{bmatrix}
            \]
            </p>

            <p class="step">7. Compute the gradient of the loss with respect to the weights of the hidden layer:</p>
            <p>Similar to step 3:</p>

            <p>
            \[
            \frac{\partial L}{\partial W_1} = \frac{\partial L}{\partial z_1} \cdot \frac{\partial z_1}{\partial W_1} = \frac{\partial L}{\partial z_1} \cdot x^T
            = \begin{bmatrix}0, 1.37, 0.857\end{bmatrix} \cdot \begin{bmatrix}0.5, 0.5\end{bmatrix}^T
            = \begin{bmatrix}0 \cdot 0.5, 1.37 \cdot 0.5, 0.857 \cdot 0.5\\0 \cdot 0.5, 1.37 \cdot 0.5, 0.857 \cdot 0.5\end{bmatrix}
            \approx \begin{bmatrix}0, 0.685, 0.428\\0, 0.685, 0.428\end{bmatrix}
            \]
            </p>

            <p class="step">8. Compute the gradient of the loss with respect to the biases of the hidden layer:</p>
            <p>Similar to step 4:</p>

            <p>
            \[
            \frac{\partial L}{\partial b_1} = \frac{\partial L}{\partial z_1} \cdot \frac{\partial z_1}{\partial b_1} = \frac{\partial L}{\partial z_1} \cdot 1
            = \frac{\partial L}{\partial z_1}
            = \begin{bmatrix}0, 1.37, 0.857\end{bmatrix}
            \]
            </p>

            <p>And that's it! We have computed the gradients of the loss function with respect to the weights and biases of the network. 
                Now, we can use these gradients to update the weights and biases using an optimization algorithm like gradient descent.</p>

            <h4>Step 6: Update Weights and Biases</h4>
            <p>We can update the weights and biases of the network using gradient descent.</p>
            <p>The update rule for gradient descent is:</p>

            <p>
            \[
            \begin{align}
            W = W - \eta \cdot \frac{\partial L}{\partial W}
            \\\\ b = b - \eta \cdot \frac{\partial L}{\partial b}
            \end{align}
            \]
            </p>
            <p>Where \( \eta \) is the learning rate.</p>
            <p>So for example, if we have a learning rate of 0.01, we can update the weights or biases as follows:</p>

            <p>
            \[
            \begin{align}
            W_1 = W_1 - 0.01 \cdot \frac{\partial L}{\partial W_1}
            = W_1 - 0.01 \cdot \begin{bmatrix}0, 0.685, 0.428\\0, 0.685, 0.428\end{bmatrix}
            = W_1 - \begin{bmatrix}0, 0.00685, 0.00428\\0, 0.00685, 0.00428\end{bmatrix}\\
            \approx \begin{bmatrix}-0.6, 0.6, 0.4\\-0.6, 0.7, 0.5\end{bmatrix} - \begin{bmatrix}0, 0.00685, 0.00428\\0, 0.00685, 0.00428\end{bmatrix}
            \approx \begin{bmatrix}-0.6, 0.59315, 0.39572\\-0.6, 0.69315, 0.49572\end{bmatrix}
            \end{align}
            \]
            </p>

            <p class="step">Step 7: Repeat</p>
            <p>Repeat the forward pass, loss calculation, backpropagation, and weight update steps for each batch of data in the training set until the epoch limit reached.</p>

            <h4>Conclusion</h4>
            <p>And there you have it.
                A simple neural network implemented from scratch using Python and NumPy.
                This is a basic example, but it covers the main calculations of how a neural network works.
            </p>
            <a href="mlp-mnist.html">In the next blog, I will show you how to build a simple MLP using PyTorch and train it on the MNIST dataset, a classic dataset for image classification.</a>
</p>
    </div>

        <p><a href="../back-to-ai-basics.html">Back to the top page of this series</a>
        <p><a href="../../index.html">Back to Home</a></p>

        <footer>
            <hr>
            <p>Made by Ethan, 2024, with 💜. <a href="mailto:phatnguyenground@gmail.com">Email me</a></p>
        </footer>
    </div>
</body>

<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
    document.addEventListener('DOMContentLoaded', function() {
      // Create mobile menu toggle button
      const sidebar = document.querySelector('.sidebar');
      const mobileMenuToggle = document.createElement('button');
      mobileMenuToggle.classList.add('mobile-menu-toggle');
      mobileMenuToggle.innerHTML = '<i class="fas fa-bars"></i>';
      
      // Add the button to the DOM
      document.body.insertBefore(mobileMenuToggle, document.body.firstChild);
      
      // Toggle sidebar visibility on mobile
      mobileMenuToggle.addEventListener('click', function() {
        sidebar.classList.toggle('sidebar-open');
        this.classList.toggle('toggle-active');
      });
      
      // Close sidebar when clicking on a link (for mobile)
      const navLinks = document.querySelectorAll('#nav-bar a');
      navLinks.forEach(link => {
        link.addEventListener('click', function() {
          if (window.innerWidth <= 768) {
            sidebar.classList.remove('sidebar-open');
            mobileMenuToggle.classList.remove('toggle-active');
          }
        });
      });
    });
</script>
<script src="../../js/scripts.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>
<script>
    // JavaScript to set the date modified
    document.getElementById('date-modified').textContent = document.lastModified;
</script>
<script>
// JavaScript to set the date modified
    document.getElementById('date-modified').textContent = document.lastModified;
    function toggleDropdown(element) {
    element.parentElement.classList.toggle("open");
  };
</script>
</html>