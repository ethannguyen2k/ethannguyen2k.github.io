<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>My Blogs - Back to AI Basics - Deeper CNN</title>
    <meta name="description" content="Explore the basics of AI with a focus on building a deeper Convolutional Neural Network (CNN) for image classification tasks.">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <link rel="stylesheet" href="../../css/blog-styles.css">
    <script>
        // Immediately apply the saved theme
        (function() {
            const savedTheme = localStorage.getItem('theme') || 'light';
            document.documentElement.setAttribute('data-theme', savedTheme);
        })();
    </script>
    <style>
        .button {
            display: inline-block;
            padding: 10px 20px;
            margin-top: 10px;
            background-color: var(--accent-color);
            color: var(--bg-primary);
            text-decoration: none;
            border-radius: 5px;
            transition: background-color 0.3s;
        }

        .button:hover {
            background-color: #0073e6;
        }
        .dropdown-heading {
            cursor: pointer;
            font-weight: bold;
            padding: 10px;
            padding-left: 30px; /* Add space for triangle on the left */
            border: 1px solid var(--border-color);
            background-color: var(--bg-secondary);
            margin: 0;
            position: relative;
        }

        /* Triangle icon using ::before pseudo-element */
        .dropdown-heading::before {
            content: '';
            border-style: solid;
            border-width: 5px 5px 0 5px; /* Triangle pointing down */
            border-color: var(--text-primary) transparent transparent transparent;
            position: absolute;
            left: 10px; /* Position triangle on the left */
            top: 50%;
            transform: translateY(-50%);
            transition: transform 0.3s; /* Smooth rotation */
        }

        .dropdown-content {
            display: none;
            padding: 10px;
            border: 1px solid #ddd;
            border-top: none;
        }

        /* Show content and rotate triangle when open */
        .dropdown.open .dropdown-content {
            display: block;
        }

        /* Rotate the triangle to point up */
        .dropdown.open .dropdown-heading::before {
            transform: translateY(-50%) rotate(180deg);
        }
    </style>
</head>

<body>
    <aside class="sidebar">
        <div class="header-content">
            <img id="logo" src="../../images/logo2.webp" alt="Logo">
            <div class="text-content">
                <h1>Hi! I'm Ethan</h1>
                <h3>Hard learning. Hardworking. Hard playing. Ready for what's next.</h3>
            </div>
        </div>
        <div class="theme-toggle">
            <button class="theme-btn" id="themeToggle" aria-label="Toggle theme">
                <i class="fa-solid fa-sun"></i>
                <i class="fa-solid fa-moon"></i>
            </button>
        </div>
        <nav id="nav-bar">
            <a href="../../index.html">Homepage</a>
            <a href="../../portfolio.html">My Portfolio</a>
            <a href="../../blog.html" class="current">My Blogs</a>
        </nav>
        <div class="sidebar-extra">
            <p><a href="mailto:phatnguyenground@gmail.com"><i class="fa-solid fa-at"></i></a>
                <a href="https://github.com/ethannguyen2k" target="_blank"><i class="fa-brands fa-github-alt"></i></a>
                <a href="https://linkedin.com/in/ethan-2k" target="_blank"><i class="fab fa-linkedin"></i></a></p>
        </div>
    </aside>

    <div class="container2 fade-in">
        <div class="blog-content">
            <h2>Back to AI Basics: Deeper Convolutional Neural Network (CNN)</h2>
            <div class="blog-info">
                <p>Date Created: <span id="date-created">May 15, 2025</span></p>
                <p>Date Modified: <span id="date-modified"></span></p>
            </div>
            <!--Change the link to code  -->
            <a target="_blank" href="https://github.com/ethannguyen2k/back-to-AI-basics/tree/main/models" class="button">Code for this topic on GitHub</a>            
            <h3>Introduction</h3>
            <p>Last time, we built a basic CNN for MNIST digit classification.
            The simplicity of MNIST allowed us to focus on the core CNN concepts without getting bogged down in complexity.
            Now, do you think it can fair well on more challenging datasets like CIFAR-10 or CIFAR-100?
            Probably not, but for the sake of learning and knowing, let's try it out.</p>

            <p>In case you missed it, here is the <a href="../back-to-ai-basics/convnet.html">previous blog post</a> where we built a basic CNN for MNIST.
            But the architecture is pretty simple: 2 convolutional blocks with 3x3 filters, ReLU activations, max pooling, and a fully connected layer, with dropout of 25%.</p>

            <h3>Basic CNN for CIFAR-10</h3>
            <img src="../back-to-ai-basics-images/cnn-architecture-diagram.svg" alt="CNN Architecture" class="blog-image" style="display: block; margin-left: auto; margin-right: auto; width: 100%;">
            <figcaption>Revisiting the last blog's architecture</figcaption>

            <p>For this task, we will only change the input channels from 1 to 3 (for RGB images) and adjust the fully connected layer input size to match CIFAR-10's spatial dimensions.</p>

            <h4>Basic CNN Architecture for CIFAR-10</h4>
            <ul>
                <li>Input channels: 1 â†’ 3 (RGB images)</li>
                <li>FC layer input: 3136 â†’ 4096 (CIFAR-10 spatial dimensions)</li>
                <li>Same depth and structure otherwise</li>
            </ul>

            <div class="code-window">
            <div class="console">
                <div class="red"></div>
                <div class="orange"></div>
                <div class="green"></div>
            </div>
        <pre class="code-content language-plaintext"><code>
============================================================
CIFAR-10 Basic CNN Results Summary
============================================================
Final Train Accuracy: 85.09%
Final Test Accuracy: 74.75%
Total Parameters: 545,098
Total Epochs: 10
Training Time: 185.73 seconds

Train Loss: 0.4164, Train Acc: 85.09%
Test Loss: 0.8065, Test Acc: 74.75%
        </code></pre>
        </div>

            <p>As we can see, even though 74.75% accuracy on CIFAR-10 is decent, it is far from usable in practice.
            Additionally, there is a significant overfitting gap of ~10% between training and test accuracy.</p>

            <h3>Improving the CNN Architecture</h3>
            <p>So in what way can we improve this basic CNN to achieve better performance on CIFAR-10?
            Yes, we can make it deeper and more complex.
            We will build a deeper CNN with more convolutional blocks, <a href="https://github.com/ethannguyen2k/back-to-AI-basics/blob/main/regularization.md#3-batch-normalization" target="_blank">batch normalization</a>, better initialization, and <a href="https://github.com/ethannguyen2k/back-to-AI-basics/blob/main/regularization.md" target="_blank">regularization techniques like dropout and data augmentation</a>.</p>

            <img src="../back-to-ai-basics-images/deeper_cnn_architecture.svg" alt="Deeper CNN Architecture" class="blog-image" style="display: block; margin-left: auto; margin-right: auto; width: 100%;">
            <figcaption>VGG-style Deeper CNN Architecture appropriate for a 10-class task</figcaption>

            <h4>Architecture Enhancements:</h4>
            <ul>
                <li>4 conv blocks (vs 2): Progressive depth 3â†’64â†’128â†’256â†’512 channels</li>
                <li>BatchNormalization: After every conv layer for stable training</li>
                <li>Better initialization: Kaiming for ReLU networks</li>
            </ul>

            <div class="dropdown">
                <h4 class="dropdown-heading" onclick="toggleDropdown(this)">What is Kaiming Initialization?</h4>
                <div class="dropdown-content">
                    <p>One of the problems with deep networks is that gradients can vanish or explode as they propagate through many layers.
                    These are problems during backpropagation in deep neural networks, especially deep feedforward or RNNs.</p>
                    <p>In short:</p>
                    <ul>
                        <li><strong>Vanishing gradients:</strong> Gradients (partial derivatives) become very small as they're propagated backward through layers, leading to slow or stalled training. Most common with sigmoid/tanh activations or very deep networks.</li>
                        <li><strong>Exploding gradients:</strong> Gradients become very large, causing weights to update too drastically and leading to instability or divergence during training. Common with RNNs over long sequences, poor weight initialization, or, of course, very deep networks.</li>
                    </ul>
                    <p>The Kaiming initialization method, also known as Kaiming He initialization or He normal initialization, is a technique for initializing the weights of artificial neural networks.</p> 
                    <p>This method was introduced in the paper titled "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification" by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.</p>
                    <p>It sets the initial weights so that the variance of the activations remains stable and consistent across layers.</p>
                    <p>The Kaiming initialization method is calculated as a random number with a Gaussian probability distribution (G) with a mean of 0.0 and a standard deviation of sqrt(2/n), where n is the number of inputs to the node.</p>
                    <p>This is different from the Xavier initialization, which uses a uniform distribution and is more suitable for tanh or sigmoid activations.</p>
                </div>
            </div>

            <h4>Regularization Techniques:</h4>
            <ul>
                <li>Data augmentation: Random crops, flips, color jitter</li>
                <li>Dropout2d: 25% in conv blocks, 50% in classifier</li>
                <li>Weight decay: L2 regularization in optimizer</li>
                <li>LR scheduling: Step decay every 5 epochs</li>
            </ul>

            <p>This is a well-structured and solid CNN architectureâ€”reminiscent of VGG-style networks but enhanced with BatchNorm after every convolutional layer, weight initialization, and a progressive increase in channel depth.
                Its strength lies in its modular design, a reasonable depth for testing, and Kaiming initialization for ReLU-based nets.
            It should perform much better on CIFAR-10 than the basic CNN.
            </p>

            <div class="code-window">
            <div class="console">
                <div class="red"></div>
                <div class="orange"></div>
                <div class="green"></div>
            </div>
        <pre class="code-content language-plaintext"><code>
============================================================
CIFAR-10 Deeper CNN Results Summary
============================================================
Final Train Accuracy: 83.24%
Final Test Accuracy: 86.45%
Best Test Accuracy: 86.45%
Total Parameters: 7,320,394
Training Time: 457.63 seconds
Overfitting Gap: -3.21%
        </code></pre>
        </div>

        <div class="code-window">
            <div class="console">
                <div class="red"></div>
                <div class="orange"></div>
                <div class="green"></div>
            </div>
        <pre class="code-content language-plaintext"><code>
============================================================
COMPARISON: Basic CNN vs Deeper CNN
============================================================
Basic CNN Test Accuracy:   74.75%
Deeper CNN Test Accuracy:  86.45%
Improvement: +11.70%
Basic CNN Parameters:      545,098
Deeper CNN Parameters:     7,320,394
Parameter Increase: 13.4x
        </code></pre>
        </div>

        <p>Okay! So we got a pretty excellent results here.
            86.45% test accuracy is solid for CIFAR-10, with only a 3.21% overfitting gap, and overall 11.7% improvement.
        </p>
        <p>It seems like BatchNorm with data augmentation really helped, the model is well regularised.
            For 15 epochs, it took about 457 seconds to train (~30s per epoch), which is reasonable, given 7.3M parameters.
        </p>

        <div class="dropdown">
            <h4 class="dropdown-heading" onclick="toggleDropdown(this)">Will Deeper CCN still work without these techniques?</h4>
            <div class="dropdown-content">
                <p>In this problem we are placing for ourselves? The performance will likely be damaged.</p>
                <ul>
                    <li>Without BatchNorm, the model may struggle with internal covariate shift.</li>
                    <li>Without Kaiming initialization, weights may be to spread out, leading to poor convergence.</li>
                    <li>Without dropout, the model is likely to overfit on CIFAR-10, especially with 7M+ parameters.</li>
                    <li>Without data augmentation, the model will be limited on data diversity.</li>
                    <li>There may even be Vanishing problem as well, though it is more prominent with sigmoid/tanh activations.</li>
                </ul>
                <p>So, in short, worse validating accuracy, massive train-test gap, internal covariate shift, diverge early.</p>
            </div>
        </div>

        <h3>Pushing Deeper CNN Further</h3>
        <p>Whenever we deal with practical model that classify images, CIFAR-10 and CIFAR-100 are always our go-to, and itt is a benchmark for many papers and models.</p>
        <p>CIFAR-10 tests "Can your model learn?"<br>
        CIFAR-100 tests "Can your model scale?"</p>
        <p>CIFAR-10 and CIFAR-100 form a somewhat natural difficulty ladder in terms of model capacity vs. task complexity.</p>
        <p>So, let's try to scale our Deeper CNN to CIFAR-100, which has 100 classes, each with 600 images, and see how it performs.
            It will be the same architecture, but the final fully connected layer will be an output of 100 classes instead of 10.</p> 

        <div class="code-window">
            <div class="console">
                <div class="red"></div>
                <div class="orange"></div>
                <div class="green"></div>
            </div>
        <pre class="code-content language-plaintext"><code>
============================================================
CIFAR-100 Deeper CNN Results Summary
============================================================
Final Train Accuracy: 46.62%
Final Test Accuracy: 53.57%
Best Test Accuracy: 53.57%
Total Parameters: 7,366,564
Total Epochs: 20
Training Time: 600.31 seconds
        </code></pre>
        </div>

        <p>Tough! The model is struggling to learn on CIFAR-100:
            53.57% test accuracy is far from usable, not to mention it is clearly underfitting.
            Sometimes, underfitting mean the model might not have enough epochs to learn yet.
        </p>

        <div class="code-window">
            <div class="console">
                <div class="red"></div>
                <div class="orange"></div>
                <div class="green"></div>
            </div>
        <pre class="code-content language-plaintext"><code>
============================================================
CIFAR-100 Deeper CNN Results Summary
============================================================
Final Train Accuracy: 51.25%
Final Test Accuracy: 57.29%
Best Test Accuracy: 57.29%
Total Parameters: 7,366,564
Total Epochs: 35
Training Time: 1093.35 seconds
        </code></pre>
        </div>

        <p>Going from 20 â†’ 35 epochs gave only ~4% gain. It's not even worth it to increase epochs at this point.</p>
        <p>So, what is the problem here?</p>
        <ol>
            <li><strong>Training Difficulty Scaling</strong>: As task complexity increases, models need to learn finer class boundaries and handle a more intra-class variance.
                This means we might have hit a bottleneck, not due to the model's architecture, but rather its capacity to learn complex features.
            </li>
            <li><strong>Diminishing Returns from Depth</strong>: Adding depth past a certain point doesn't yield better performance, unless architecture supports it (e.g., residuals).
                So with an even deeper network, we might not see significant improvements.
            </li>
        </ol>
        <p>Deeper networks help only up to the point they can be optimized and generalize. 
            When task difficulty scales up and you don't adjust other factors (like width, augmentation, learning rate, architecture), depth alone gives less return.
        </p>

        <p>To go further, we will need more than just depth.
            We will need an improved architecture.
        </p>
        <img src="https://i.imgflip.com/11apvu.jpg" alt="We Need MORE FIREPOWER!" class="blog-image" style="display: block; margin-left: auto; margin-right: auto; width: 50%;">
        <figcaption>We need more firepower!</figcaption>
        
        <h3>Introducing Modern CNN Architectures</h3>
        <p>As of 2025, CNNs are still widely used, but their dominance has been overtaken in many areasâ€”especially vision tasksâ€”by Vision Transformers (ViTs) and hybrid models. 
            That said, CNNs remain efficient and preferred for some real-time and resource-constrained tasks.
        </p>
        <ul>
            <li>CNNs (like EfficientNet, ConvNeXt) are still faster and lighter than ViTs for many edge/real-time tasks.</li>
            <li>ViTs (like Swin Transformer, DeiT, and SAM backbones) are state-of-the-art in accuracy for vision benchmarks but need more compute. 
                They are more popular in research and large-scale industry models.</li>
            <li>Hybrid architectures (e.g., CoaT, LeViT) combine CNNs + attention â€” best of both worlds in some tasks.</li>
        </ul>
        <p>The CNN is still a vast space. 
            You can check out more from <a href="https://docs.pytorch.org/vision/stable/models.html" target="_blank">PyTorch models and pre-trained weights</a>
            or <a href="https://keras.io/api/applications/" target="_blank">Keras applications</a>.
            You can also check out <a href="https://huggingface.co/docs/transformers/model_doc/vit" target="_blank">Hugging Face Vision Transformers</a> for ViTs, hybrid models, and possibly more.
        </p>
        <p>However, for this blog, we will focus on a handful of architectures, starting with...</p>

            <h3>Residual Network (ResNet)</h3>
            <p>One of the common problems with deep CNNs when adding more layers is the Vanishing/Exploding gradient problem.
            This makes it hard to train very deep networks, as gradients can become too small or too large, leading to poor convergence or divergence, which also leads to high training and test loss.
            </p>
            <p>Proposed in 2015, researchers at Microsoft Research introduced a new architecture called Residual Network, or ResNet.
            </p>
            <p>ResNet is all about minimising the Vanishing/Exploding gradient problem by introducing a core idea called <strong>skip connections</strong> (or residual connections) that allow gradients to flow more easily through the network.
                Instead of learning a direct mapping H(x), ResNet learns the residual mapping F(x) = H(x) - x, where x is the input.
            </p>
            <div class="dropdown">
                <h4 class="dropdown-heading" onclick="toggleDropdown(this)">What is a Residual, or Residual Block?</h4>
                <div class="dropdown-content">
                    <p>At its core, a residual block is a neural module that computes:</p>
                    <p>
                    \[
                    \text{output} = F(x) + x
                    \]
                    </p>
                    <p>Where:</p>
                    <ul>
                        <li><code>x</code> is the original input to the block</li>
                        <li><code>F(x)</code> is the output of the small neural network (usually a few conv + BN + ReLU layers)</li>
                    </ul>
                    <p>x is added back in via an identity (skip) connection. This skip path bypasses the main transformation.</p>
                    <p>So, the residual block learns the difference between the input and output (i.e., the residual), which is easier, rather than trying to learn the output directly.</p>
                    <p>
                    \[
                    F(x) = H(x) - x \implies H(x) = F(x) + x
                    \]
                    </p>
                    <p>This is important in very deep network, where layers may not be needed to transform data significantly, 
                        or the identity mappings might already be sufficient in parts of the network.
                    </p>
                    <p>In giving the models a shortcut path, it has the flexibility to leave the data untouched (learn F(x)=0) or apply a transformation (learn F(x)â‰ 0) as needed.
                    </p>
                    <p>Think of it like this: Traditional layers try to mutate data every step - even when it doesn't help.
                        Residual blocks can let data pass through unmodified if needed, and only nudge it when helpful.
                        This can help to avoid degradation in performance as models go deeper.
                    </p>
                </div>
            </div>
            <p>There are multiple variations of ResNet with lots of layers.
                We will choose 18 layers since the purpose of this blog is to discuss the underlying concept, or ResNet-18 to be precise.</p>
            <img src="https://www.researchgate.net/profile/Chih-Hsien-Hsia/publication/353026381/figure/fig2/AS:1042698844770347@1625610185976/Color-online-ResNet-18-architecture.ppm" alt="ResNet-18 Architecture" class="blog-image" style="display: block; margin-left: auto; margin-right: auto; width: 100%;">
            <figcaption>ResNet-18 Architecture (<a href="https://www.researchgate.net/figure/Color-online-ResNet-18-architecture_fig2_353026381" target="_blank">Source</a>)</figcaption>
            <h4>Key ResNet-18 Features:</h4>
            <ul>
                <li>Skip connections: <code>out += self.shortcut(x)</code> - the core innovation</li>
                <li>18 layers: 2+2+2+2 BasicBlocks + initial conv + final FC</li>
                <li>Efficient: ~11M parameters vs 7.3M custom CNN</li>
                <li>SGD + momentum: Better for ResNet training than Adam</li>
            </ul>

            <p>Now, we have 2 ways to get ResNet-18: build it from scratch or use a pre-trained model.
                To use a pre-trained model, you can invoke a simple function below.
                For building from scratch, it is demonstrated in my <a href="https://github.com/ethannguyen2k/back-to-AI-basics/blob/main/models/cifar100_resnet.ipynb" target="_blank">notebook</a>, feel free to poke around. 
            </p>

            <div class="code-window">
            <div class="console">
                <div class="red"></div>
                <div class="orange"></div>
                <div class="green"></div>
            </div>
        <pre class="code-content language-python"><code>
from torchvision.models import resnet18, ResNet18_Weights

# Load model with pretrained weights
weights = ResNet18_Weights.DEFAULT
model = resnet18(weights=weights)
        </code></pre>
        </div>

        <p>Though you might be careful with using the pre-trained model above.
            The weights that specified is used for ImageNet, which is 224x224 high-res natural images dataset, completely different from 32x32 low-res images CIFAR dataset.
            You will might want to modify the fist conv layer and use smaller learning rates if you want to use it.
            Anyhow, here is the building from scratch performance:
        </p>

        <h4>ResNet-18 on CIFAR-10</h4>
            <div class="code-window">
            <div class="console">
                <div class="red"></div>
                <div class="orange"></div>
                <div class="green"></div>
            </div>
        <pre class="code-content language-plaintext"><code>
============================================================
ResNet-18 on CIFAR10 Results Summary
============================================================
Final Train Accuracy: 91.77%
Final Test Accuracy: 88.75%
Best Test Accuracy: 88.84%
Total Parameters: 11,173,962
Total Epochs: 15
Training Time: 545.73 seconds
Overfitting Gap: 3.02%
        </code></pre>
        </div>

            <div class="code-window">
            <div class="console">
                <div class="red"></div>
                <div class="orange"></div>
                <div class="green"></div>
            </div>
        <pre class="code-content language-plaintext"><code>
============================================================
COMPARISON: Previous CNN vs ResNet-18 on CIFAR10
============================================================
Deeper CNN Test Accuracy:  86.45%
ResNet-18 Test Accuracy:   88.84%
Improvement: +2.39%
Deeper CNN Parameters:     7,320,394
ResNet-18 Parameters:      11,173,962
Parameter Efficiency: 1.53x
        </code></pre>
        </div>

        <h4>ResNet-18 on CIFAR-100</h4>
            <div class="code-window">
            <div class="console">
                <div class="red"></div>
                <div class="orange"></div>
                <div class="green"></div>
            </div>
        <pre class="code-content language-plaintext"><code>
============================================================
ResNet-18 on CIFAR100 Results Summary
============================================================
Final Train Accuracy: 84.64%
Final Test Accuracy: 71.53%
Best Test Accuracy: 71.75%
Total Parameters: 11,220,132
Total Epochs: 25
Training Time: 893.19 seconds
Overfitting Gap: 13.11%
        </code></pre>
        </div>

            <div class="code-window">
            <div class="console">
                <div class="red"></div>
                <div class="orange"></div>
                <div class="green"></div>
            </div>
        <pre class="code-content language-plaintext"><code>
============================================================
COMPARISON: Previous CNN vs ResNet-18 on CIFAR100
============================================================
Deeper CNN Test Accuracy:  53.57%
ResNet-18 Test Accuracy:   71.75%
Improvement: +18.18%
Deeper CNN Parameters:     7,366,564
ResNet-18 Parameters:      11,220,132
Parameter Efficiency: 1.53x
        </code></pre>
        </div>

        <p>Okay, we got some good insights here:</p>
        <ul>
            <li>Both models perform well on simpler 10-class problem, with only 2.39% increase.
                We can't really tell if skip connections method provide a benefit here or just a good weights initialization.
            </li>
            <li>For 100-class problem, ResNet-18 outperforms the previous Deeper CNN by 18.18%!
                We have already said that the previous deeper CNN can't handle the complex gradient flow issues.
                ResNet, with 1.53x more parameters, its identity mappings has delivered a better result, and potentially even better with more epochs.
            </li>
        </ul>

        <h3>Conclusion</h3>
        <p>It is no doubt that for understanding and exploring, the simplest of CNNs can give you a glimpse of how convolutional neural networks work and how it benefits the image classification domains, and possibly more.
            However, as we have seen, the more complex imageries, or bigger the number of pixels, the harder for these basics models to comprehend.
            The deeper CNNs with BatchNorm, Kaiming initialization, and regularization techniques can help us achieve better performance on more complex imageries.
            We also get to know a new architecture called ResNet, which is a great example of how to solve the vanishing/exploding gradient problem in deep networks.
        </p>

        <p>In the next part of this Deep CNNs series, we will continue to explore other modern CNNs and its effectiveness in its own areas.</p>
        <section id="references">
            <h2>References</h2>
            <ol>
                <li>He, K., Zhang, X., Ren, S., & Sun, J. (2015). Deep Residual Learning for Image Recognition. arXiv:1512.03385v1. Retrieved from <a href="https://arxiv.org/pdf/1512.03385" target="_blank">https://arxiv.org/pdf/1512.03385</a></li>
                <li>PyTorch. (n.d.). torchvision.models.resnet18. Retrieved from <a href="https://docs.pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html" target="_blank">https://docs.pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html</a></li>
                <li>GeeksforGeeks. (2025). Kaiming Initialization in Deep Learning. Last Updated: 02 Apr, 2025. Retrieved from <a href="https://www.geeksforgeeks.org/kaiming-initialization-in-deep-learning/" target="_blank">https://www.geeksforgeeks.org/kaiming-initialization-in-deep-learning/</a></li>
                <li>GeeksforGeeks. (2025). Residual Networks (ResNet) - Deep Learning. Last Updated: 07 Apr, 2025. Retrieved from <a href="https://www.geeksforgeeks.org/residual-networks-resnet-deep-learning/" target="_blank">https://www.geeksforgeeks.org/residual-networks-resnet-deep-learning/</a></li>
                <li>GeeksforGeeks. (2025). ResNet18 from Scratch Using PyTorch. Last Updated: 21 Apr, 2025. Retrieved from <a href="https://www.geeksforgeeks.org/resnet18-from-scratch-using-pytorch/" target="_blank">https://www.geeksforgeeks.org/resnet18-from-scratch-using-pytorch/</a></li>
            </ol>
        </section>  
    
        </div>
        <p><a href="../back-to-ai-basics.html">Back to the top page of this series</a>
        <p><a href="../../index.html">Back to Home</a></p>

        <footer>
            <hr>
            <p>Made by Ethan, 2024, with ðŸ’œ. <a href="mailto:phatnguyenground@gmail.com">Email me</a></p>
        </footer>
    </div>
</body>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
    document.addEventListener('DOMContentLoaded', function() {
      // Create mobile menu toggle button
      const sidebar = document.querySelector('.sidebar');
      const mobileMenuToggle = document.createElement('button');
      mobileMenuToggle.classList.add('mobile-menu-toggle');
      mobileMenuToggle.innerHTML = '<i class="fas fa-bars"></i>';
      
      // Add the button to the DOM
      document.body.insertBefore(mobileMenuToggle, document.body.firstChild);
      
      // Toggle sidebar visibility on mobile
      mobileMenuToggle.addEventListener('click', function() {
        sidebar.classList.toggle('sidebar-open');
        this.classList.toggle('toggle-active');
      });
      
      // Close sidebar when clicking on a link (for mobile)
      const navLinks = document.querySelectorAll('#nav-bar a');
      navLinks.forEach(link => {
        link.addEventListener('click', function() {
          if (window.innerWidth <= 768) {
            sidebar.classList.remove('sidebar-open');
            mobileMenuToggle.classList.remove('toggle-active');
          }
        });
      });
    });
</script>
<script src="../../js/scripts.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>
<script>
    // JavaScript to set the date modified
    document.getElementById('date-modified').textContent = document.lastModified;
</script>
<script>
// JavaScript to set the date modified
    document.getElementById('date-modified').textContent = document.lastModified;
    function toggleDropdown(element) {
    element.parentElement.classList.toggle("open");
  };
</script>
</html>